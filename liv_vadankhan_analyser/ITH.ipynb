{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import cheby1, filtfilt, savgol_filter\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "\n",
    "\n",
    "# Move to the root directory\n",
    "\n",
    "\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "\n",
    "# Add the root directory to the system path\n",
    "\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "\n",
    "# Import the importlib module\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "# import function implementations\n",
    "import stst_urls\n",
    "\n",
    "\n",
    "# Reload the modules\n",
    "\n",
    "\n",
    "importlib.reload(stst_urls)\n",
    "\n",
    "\n",
    "# Re-import the functions\n",
    "\n",
    "\n",
    "from stst_urls import GTX_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileserver link: https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/\n",
      "['https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI0M/LIV_54_QCI0M_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI2K/LIV_53_QCI2K_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI2T/LIV_54_QCI2T_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI2V/LIV_54_QCI2V_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI2X/LIV_53_QCI2X_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI3E/LIV_54_QCI3E_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI3L/LIV_53_QCI3L_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI12/LIV_53_QCI12_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI33/LIV_53_QCI33_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI46/LIV_54_QCI46_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI48/LIV_53_QCI48_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv']\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\n",
    "    \"QCI12\",\n",
    "    \"QCI2X\",\n",
    "    \"QCI2K\",\n",
    "    \"QCI2T\",\n",
    "    \"QCI48\",\n",
    "    \"QCI2V\",\n",
    "    \"QCI0M\",\n",
    "    \"QCI46\",\n",
    "    \"QCI3L\",\n",
    "]  # List of wafer codes\n",
    "\n",
    "# Recent Good Wafers (non retest SQL check)\n",
    "# QCI12\n",
    "# QCI2X\n",
    "# QCI2K\n",
    "# QCI2T\n",
    "# QCI48\n",
    "# QCI2V\n",
    "# QCI0M\n",
    "# QCI46\n",
    "# QCI33\n",
    "# QCI3L\n",
    "# QCI3E\n",
    "\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"Mechanistic_Port_Study\"\n",
    "\n",
    "DECODER_FILE = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "DECODER_FILE_PATH = ROOT_DIR / \"decoders\" / DECODER_FILE\n",
    "RESULTS_FILE_PATH = ROOT_DIR / \"results\"\n",
    "\n",
    "EXPORTS_FILEPATH = ROOT_DIR / \"exports\"\n",
    "# Create the exports folder if it doesn't exist\n",
    "if not os.path.exists(EXPORTS_FILEPATH):\n",
    "    os.makedirs(EXPORTS_FILEPATH)\n",
    "# print(EXPORTS_FILEPATH)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def liv_raw_filelink_finder(wafer_codes, fileserver_link: str, product_code=\"QC\"):\n",
    "    fileserver_link = f\"{fileserver_link}{product_code}/\"\n",
    "    print(f\"fileserver link: {fileserver_link}\")\n",
    "\n",
    "    response = requests.get(fileserver_link, verify=False)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    subdirectory_urls = []\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        if href and any(wafer_code in href for wafer_code in wafer_codes):\n",
    "            subdirectory_urls.append(fileserver_link + href)\n",
    "\n",
    "    # RAW file tracking\n",
    "    file_urls = []\n",
    "    file_cod_urls = []\n",
    "    file_degradation_urls = []\n",
    "    file_times = []\n",
    "    file_cod_times = []\n",
    "    file_degradation_times = []\n",
    "\n",
    "    machine_list = []\n",
    "    machine_dict = {}\n",
    "\n",
    "    # Processed COD tracking\n",
    "    processed_cod70_urls = []\n",
    "    processed_cod250_urls = []\n",
    "    processed_cod_base_urls = []\n",
    "\n",
    "    for wafer_code, subdirectory_url in zip(wafer_codes, subdirectory_urls):\n",
    "        response = requests.get(subdirectory_url, verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "\n",
    "        latest_file = None\n",
    "        latest_cod_file = None\n",
    "        latest_degradation_file = None\n",
    "        latest_time = \"\"\n",
    "        latest_cod_time = \"\"\n",
    "        latest_degradation_time = \"\"\n",
    "        machine_name = None\n",
    "\n",
    "        # Processed COD placeholders\n",
    "        proc_cod70 = None\n",
    "        proc_cod250 = None\n",
    "        proc_cod_base = None\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "\n",
    "            # RAW file logic\n",
    "            if \"RAW\" in href:\n",
    "                time_str = href[-18:-4]  # Timestamp from filename\n",
    "\n",
    "                if not machine_name:\n",
    "                    machine_name = href[:6]  # Extract machine name\n",
    "\n",
    "                if \"COD250\" in href:\n",
    "                    if time_str > latest_cod_time:\n",
    "                        latest_cod_time = time_str\n",
    "                        latest_cod_file = subdirectory_url + href\n",
    "                elif \"COD70\" in href:\n",
    "                    if time_str > latest_degradation_time:\n",
    "                        latest_degradation_time = time_str\n",
    "                        latest_degradation_file = subdirectory_url + href\n",
    "                else:\n",
    "                    if time_str > latest_time:\n",
    "                        latest_time = time_str\n",
    "                        latest_file = subdirectory_url + href\n",
    "\n",
    "            # Processed COD logic (only pick the first match for each type)\n",
    "            elif \"processed\" in href and \"COD\" in href:\n",
    "                full_url = subdirectory_url + href\n",
    "                if \"COD250\" in href and proc_cod250 is None:\n",
    "                    proc_cod250 = full_url\n",
    "                elif \"COD70\" in href and proc_cod70 is None:\n",
    "                    proc_cod70 = full_url\n",
    "                elif \"COD\" in href and \"COD250\" not in href and \"COD70\" not in href and proc_cod_base is None:\n",
    "                    proc_cod_base = full_url\n",
    "\n",
    "        # Append results\n",
    "        if latest_file:\n",
    "            file_urls.append(latest_file)\n",
    "            file_times.append(latest_time)\n",
    "        if latest_cod_file:\n",
    "            file_cod_urls.append(latest_cod_file)\n",
    "            file_cod_times.append(latest_cod_time)\n",
    "        if latest_degradation_file:\n",
    "            file_degradation_urls.append(latest_degradation_file)\n",
    "            file_degradation_times.append(latest_degradation_time)\n",
    "        if machine_name:\n",
    "            machine_list.append(machine_name)\n",
    "            machine_dict[wafer_code] = machine_name\n",
    "\n",
    "        # Append processed CODs\n",
    "        processed_cod70_urls.append(proc_cod70)\n",
    "        processed_cod250_urls.append(proc_cod250)\n",
    "        processed_cod_base_urls.append(proc_cod_base)\n",
    "\n",
    "    return (\n",
    "        file_urls,\n",
    "        file_cod_urls,\n",
    "        file_degradation_urls,\n",
    "        file_times,\n",
    "        file_cod_times,\n",
    "        file_degradation_times,\n",
    "        machine_list,\n",
    "        machine_dict,\n",
    "        processed_cod70_urls,\n",
    "        processed_cod250_urls,\n",
    "        processed_cod_base_urls,\n",
    "    )\n",
    "\n",
    "\n",
    "# Calling code\n",
    "(\n",
    "    file_urls,\n",
    "    file_cod_urls,\n",
    "    file_degradation_urls,\n",
    "    file_times,\n",
    "    file_cod_times,\n",
    "    file_degradation_times,\n",
    "    machine_list,\n",
    "    machine_dict,\n",
    "    processed_cod70_urls,\n",
    "    processed_cod250_urls,\n",
    "    processed_cod_base_urls,\n",
    ") = liv_raw_filelink_finder(wafer_codes, GTX_URL, \"QC\")\n",
    "print(processed_cod_base_urls)\n",
    "\n",
    "# DEBUG: INPUT LINKS TO OTHER GTX FILES HERE\n",
    "# file_urls = [\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_DNS-LIVTKCOD_LCRVCOD250-DNS_RAW20250227044906.CSV\",\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_LIVBLTKCOD_COD250-DNS_RAW20250228082707.CSV\",\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_LIVBLTKCOD_COD250-DNS_RAW20250311164324.CSV\",\n",
    "# ]\n",
    "# print(file_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data to Desired Raw Sweep Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selects required columns\n",
    "- transposes\n",
    "- stacks data in tall format\n",
    "- adds in device coords from decoder file\n",
    "- loops for every csv file chosen, and stores raw_sweep dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 10.65 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.01 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.39 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 12.10 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 9.83 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.01 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.02 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.02 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.33 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 11.24 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 11.48 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.02 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.00 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.87 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 13.40 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 10.94 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.00 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.00 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.54 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 12.52 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 11.62 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.02 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.01 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.02 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.01 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.02 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 2.46 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 14.18 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 10.67 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.01 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.05 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.55 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 12.31 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 9.34 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.41 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.20 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 10.98 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 11.90 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.02 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.27 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 13.22 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 4.70 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.01 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.00 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 16\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.00 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 0.93 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.01 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 5.67 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 11.41 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.00 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.53 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 12.98 seconds\n",
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 10.70 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.04 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.69 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 12.47 seconds\n"
     ]
    }
   ],
   "source": [
    "def transform_raw_liv_file(file_url, decoder_file_path, machine_code, wafer_id):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the CSV file from the URL, skipping the first 19 rows\n",
    "    print(\"Step 1: Reading the CSV file...\")\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        skiprows=19,\n",
    "    )\n",
    "    print(f\"Step 1 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 3: Subsetting the data frame...\")\n",
    "    col_names = df.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Transpose the data frame and reset index\n",
    "    print(\"Step 4: Transposing the data frame...\")\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 5: Splitting the transposed table...\")\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Learn data dimensions\n",
    "    print(\"Step 6: Learning data dimensions...\")\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Concatenate all Voltage columns into one\n",
    "    print(\"Step 7: Concatenating Voltage columns...\")\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = [i // n_meas + 1 for i in range(n_meas * n_devices)]\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all PD columns into one\n",
    "    print(\"Step 8: Concatenating PD columns...\")\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 9: Performing Cartesian join...\")\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Add device coordinates from original RAW file\n",
    "    print(\"Step 10: Adding device coordinates...\")\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(\n",
    "            df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 11: Merging with decoder file...\")\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Rename the columns\n",
    "    print(\"Step 12: Renaming columns...\")\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 13: Adding current column...\")\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 14: Adding WAFER_ID column...\")\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 15: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "def transform_raw_liv_file_first_n_rows(file_url, decoder_file_path, machine_code, wafer_id, n):\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    # Step 2: Read the data rows, skipping the header rows\n",
    "    print(\"Step 2: Reading the data rows, skipping the header rows...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_url, skiprows=19, nrows=n)\n",
    "    print(f\"Step 2 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 3: Subsetting the data frame...\")\n",
    "    start_time = time.time()\n",
    "    col_names = df.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Transpose the data frame and reset index\n",
    "    print(\"Step 4: Transposing the data frame...\")\n",
    "    start_time = time.time()\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 5: Splitting the transposed table...\")\n",
    "    start_time = time.time()\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Learn data dimensions\n",
    "    print(\"Step 6: Learning data dimensions...\")\n",
    "    start_time = time.time()\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Concatenate all Voltage columns into one\n",
    "    print(\"Step 7: Concatenating Voltage columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = [i // n_meas + 1 for i in range(n_meas * n_devices)]\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all PD columns into one\n",
    "    print(\"Step 8: Concatenating PD columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 9: Performing Cartesian join...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Add device coordinates from original RAW file\n",
    "    print(\"Step 10: Adding device coordinates...\")\n",
    "    start_time = time.time()\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(\n",
    "            df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 11: Merging with decoder file...\")\n",
    "    start_time = time.time()\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Rename the columns\n",
    "    print(\"Step 12: Renaming columns...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 13: Adding current column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 14: Adding WAFER_ID column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 15: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time_overall\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    sampling_rate = 1\n",
    "\n",
    "    return (df_raw_sweeps, n_meas, n_devices, sampling_rate)\n",
    "\n",
    "\n",
    "def transform_raw_liv_file_every_nth_laser(file_url, decoder_file_path, machine_code, wafer_id, n=10000):\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    # Step 2: Read the data rows, skipping the header rows\n",
    "    print(\"Step 2: Reading the data rows, skipping the header rows...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_url, skiprows=19)\n",
    "    print(f\"Step 2 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Filter every nth laser based on TOUCHDOWN\n",
    "    print(f\"Step 3: Filtering every {n}th laser...\")\n",
    "    start_time = time.time()\n",
    "    df_filtered = df[df[\"TOUCHDOWN\"] % n == 0]\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 4: Subsetting the data frame...\")\n",
    "    start_time = time.time()\n",
    "    col_names = df_filtered.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df_filtered[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Transpose the data frame and reset index\n",
    "    print(\"Step 5: Transposing the data frame...\")\n",
    "    start_time = time.time()\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 6: Splitting the transposed table...\")\n",
    "    start_time = time.time()\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Learn data dimensions\n",
    "    print(\"Step 7: Learning data dimensions...\")\n",
    "    start_time = time.time()\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all Voltage columns into one\n",
    "    print(\"Step 8: Concatenating Voltage columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    # Instead of generating a new TOUCHDOWN, reuse the one from df_filtered\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = df_filtered[\"TOUCHDOWN\"].repeat(n_meas).values\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Concatenate all PD columns into one\n",
    "    print(\"Step 9: Concatenating PD columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 10: Performing Cartesian join...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Add device coordinates from original RAW file\n",
    "    print(\"Step 11: Adding device coordinates...\")\n",
    "    start_time = time.time()\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(\n",
    "            df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 12: Merging with decoder file...\")\n",
    "    start_time = time.time()\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Rename the columns\n",
    "    print(\"Step 13: Renaming columns...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 14: Adding current column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 15: Adding WAFER_ID column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 16: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 16: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 16 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time_overall\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    sampling_rate = n\n",
    "\n",
    "    return (df_raw_sweeps, n_meas, n_devices, sampling_rate)\n",
    "\n",
    "\n",
    "raw_sweeps_tables = []\n",
    "device_numbers = []\n",
    "sampling_rates = []\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROW_NUMBER = 1000\n",
    "\n",
    "# # CAL\n",
    "# for file_url, machine_code in zip(file_urls, machine_list):\n",
    "#     df_raw_sweeps, n_meas, n_devices = transform_raw_liv_file_first_n_rows(file_url, DECODER_FILE_PATH, machine_code, ROW_NUMBER)\n",
    "#     if df_raw_sweeps[\"TE_LABEL\"].isna().any():\n",
    "#         raise ValueError(\"ERROR: Decoder Matching Failed! Perhaps the wrong decoder file was used, no matching X or Y coords found\")\n",
    "#     raw_sweeps_tables.append(df_raw_sweeps)\n",
    "#     device_numbers.append(n_devices)\n",
    "\n",
    "# # Display the first 10 rows of the raw_sweeps table\n",
    "# print(raw_sweeps_tables[0].head(10))\n",
    "# print(device_numbers[0])\n",
    "\n",
    "# DEBUG: CALLING SAMPLED DATA ACROSS MULTIPLE WAFERS\n",
    "SAMPLE_ROWS = 10000  # You can change this value to any other number as needed\n",
    "for file_url, machine_code, wafer_code in zip(file_urls, machine_list, wafer_codes):\n",
    "    df_raw_sweeps, n_meas, n_devices, sampling_rate = transform_raw_liv_file_every_nth_laser(\n",
    "        file_url, DECODER_FILE_PATH, machine_code, wafer_code, n=SAMPLE_ROWS\n",
    "    )\n",
    "    if df_raw_sweeps[\"TE_LABEL\"].isna().any():\n",
    "        raise ValueError(\n",
    "            \"ERROR: Decoder Matching Failed! Perhaps the wrong decoder file was used, no matching X or Y coords found\"\n",
    "        )\n",
    "    raw_sweeps_tables.append(df_raw_sweeps)\n",
    "    device_numbers.append(n_devices)\n",
    "    sampling_rates.append(sampling_rate)\n",
    "\n",
    "# # Concatenate all dataframes together\n",
    "# df_combined = pd.concat(raw_sweeps_tables, ignore_index=True)\n",
    "# # easy measure to make rest of code call:\n",
    "# raw_sweeps_tables = [df_combined]\n",
    "\n",
    "# # Display the first 10 rows of the combined dataframe\n",
    "# print(df_combined.head(10))\n",
    "# print(raw_sweeps_tables[0].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some predefined functions that can be used outside of ITH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_sweep_analysis(df):\n",
    "    \"\"\"\n",
    "    Compute first and second order differentials for voltage (Vf) and photodiode signal (PD)\n",
    "    while ensuring calculations remain per device.\n",
    "    Additionally, compute min and max PD per touchdown and clone max PD across the sweep.\n",
    "    \"\"\"\n",
    "    df[\"dV/dI\"] = df.groupby(\"TOUCHDOWN\")[\"Vf\"].diff()\n",
    "    df[\"dP/dI\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].diff()\n",
    "    df[\"d2V/dI2\"] = df.groupby(\"TOUCHDOWN\")[\"dV/dI\"].diff()\n",
    "    df[\"d2P/dI2\"] = df.groupby(\"TOUCHDOWN\")[\"dP/dI\"].diff()\n",
    "\n",
    "    df[\"MAX_PD\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].transform(\"max\")\n",
    "    df[\"MIN_PD\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].transform(\"min\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def flag_no_laser_touchdowns(df_raw_sweeps):\n",
    "    \"\"\"\n",
    "    Adds a \"FLAG\" column to df_raw_sweeps, labeling touchdowns as \"NO LASER\"\n",
    "    if the max PD value for that touchdown is below 1.\n",
    "    \"\"\"\n",
    "    df_raw_sweeps[\"FLAG\"] = np.nan\n",
    "    no_laser_touchdowns = df_raw_sweeps.groupby(\"TOUCHDOWN\")[\"PD\"].max()\n",
    "    no_laser_touchdowns = no_laser_touchdowns[no_laser_touchdowns < 1].index\n",
    "    df_raw_sweeps.loc[df_raw_sweeps[\"TOUCHDOWN\"].isin(no_laser_touchdowns), \"FLAG\"] = \"NO LASER\"\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "# Linear model for line fitting\n",
    "def linear_model(x, slope, intercept):\n",
    "    return slope * x + intercept\n",
    "\n",
    "\n",
    "# Least Absolute Residuals fitting function using L1 norm\n",
    "def least_absolute_residuals_fit(x, y, model, initial_guess, bounds):\n",
    "    def objective(params):\n",
    "        return np.sum(np.abs(model(x, *params) - y))\n",
    "\n",
    "    result = minimize(objective, initial_guess, bounds=bounds, method=\"L-BFGS-B\", options={\"maxiter\": 1000})\n",
    "\n",
    "    residuals = np.abs(model(x, *result.x) - y)\n",
    "    mean_abs_error = np.mean(residuals)\n",
    "\n",
    "    return result.x, mean_abs_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I_th Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Has a function that finds ith given an input intensity and current array\n",
    "    - **NB:** find_ith_value_labview is trying to mimic labview more closely, while the find_ith_value is a custom function that finds ith value using a similar but not identical mechanisim\n",
    "- Calls the find_ith_value function on the multiple lasers in the raw sweep file with evaluate_ITH_on_rawsweep\n",
    "- Then generates a device level summary using generate_ITH_device_summary_table\n",
    "- calls this code and exports in the main for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008731437276483298\n",
      "2.2574539834284877e-06\n",
      "0.0009145692394722243\n",
      "2.86357953759205e-05\n",
      "0.0008810003739885495\n",
      "8.372825356260545e-06\n",
      "0.0008534878434638233\n",
      "5.724036031654619e-06\n",
      "0.0008087252382313131\n",
      "5.759119542506788e-06\n",
      "0.0008277020725579558\n",
      "4.259888201568181e-06\n",
      "0.0008085335683683614\n",
      "3.846608894639682e-06\n",
      "0.000868165341482489\n",
      "3.7113987947258524e-06\n",
      "0.0008833977455428158\n",
      "5.424060626892032e-06\n",
      "0.0008779046642819822\n",
      "4.499789639785524e-06\n",
      "0.0008860664053302552\n",
      "4.503549483407212e-06\n",
      "0.0008640016799851126\n",
      "4.3114665451855275e-06\n",
      "0.0008321530536691177\n",
      "4.154779694887436e-06\n",
      "0.0008108947779085842\n",
      "3.6732867138594526e-06\n",
      "0.0008293030099635619\n",
      "6.354452643144125e-06\n",
      "0.0008066921434381432\n",
      "3.907550742131846e-06\n",
      "0.0008261367976702505\n",
      "6.246465018449108e-06\n",
      "0.0008646782436562801\n",
      "4.690851176250807e-06\n",
      "0.0007942016168629002\n",
      "1.424599925824072e-06\n",
      "0.0008578473412061647\n",
      "3.4707836390993637e-06\n",
      "0.0009018511108209389\n",
      "5.3486316569471586e-06\n",
      "0.0008376962273484996\n",
      "3.277720568838736e-06\n",
      "0.0008345487693768067\n",
      "3.7461399237275578e-06\n",
      "0.0008206316436329368\n",
      "6.3017836227658606e-06\n",
      "0.0007891436601772317\n",
      "2.9368190405521736e-06\n",
      "0.000831784492094328\n",
      "5.863655504438185e-06\n",
      "0.0008750710003922747\n",
      "7.4481896710454065e-06\n",
      "0.0008564772389679929\n",
      "4.234156613996499e-06\n",
      "0.0008803379727134536\n",
      "6.760788999305706e-06\n",
      "0.0008722329025388878\n",
      "3.6260705060516307e-06\n",
      "0.0008139365329436356\n",
      "1.909718152545866e-06\n",
      "0.0008402066619054584\n",
      "4.1671280005353284e-06\n",
      "0.0008390338691008006\n",
      "2.90010177403244e-06\n",
      "0.000796631658167786\n",
      "2.4876218974109662e-06\n",
      "0.0008444667544886873\n",
      "8.36579937656756e-06\n",
      "0.0008817356461917709\n",
      "8.311950689283348e-06\n",
      "0.0008382156515850218\n",
      "5.246408393299871e-06\n",
      "0.0008419770701121128\n",
      "6.4173130020438305e-06\n",
      "0.0008187040001315273\n",
      "5.135621132729125e-06\n",
      "0.0008005192179886451\n",
      "5.936008404280003e-06\n",
      "0.0008048727534060921\n",
      "4.963349493150929e-06\n",
      "0.0008046271444071218\n",
      "6.89189520257999e-06\n",
      "0.0007819414009114166\n",
      "6.285979496092957e-06\n",
      "0.0007957079822600472\n",
      "5.898370719486596e-06\n",
      "0.0008249717089700428\n",
      "7.952818833736424e-06\n",
      "0.0007890563413807897\n",
      "5.842431417449248e-06\n",
      "0.0008227490209175328\n",
      "5.3277662091136e-06\n",
      "0.0008018178891730302\n",
      "6.450242086222547e-06\n",
      "0.0008634323489155142\n",
      "5.670197443086087e-06\n",
      "0.0008292850206775175\n",
      "8.227631806369078e-06\n",
      "0.0008518643648311475\n",
      "5.439255198995407e-06\n",
      "0.0008276694541007771\n",
      "6.19935422753826e-06\n",
      "0.0008264720900201164\n",
      "8.367108544055766e-06\n",
      "0.0008783002311900707\n",
      "6.858108485936537e-06\n",
      "0.0009097751629551364\n",
      "7.612254569352143e-06\n",
      "0.0008351655525896581\n",
      "4.328442589237185e-06\n",
      "0.0008001462085082994\n",
      "5.989453124248091e-06\n",
      "0.0008861461207840387\n",
      "7.755250928124451e-06\n",
      "0.0008340037791718832\n",
      "7.27790666216142e-06\n",
      "0.0008868236950386688\n",
      "7.684094269924569e-06\n",
      "0.0008812740818769326\n",
      "7.60098601774801e-06\n",
      "0.0008357469767963057\n",
      "4.230044942916744e-06\n",
      "0.0008492794324405889\n",
      "5.878094682012477e-06\n",
      "0.0008086244850638614\n",
      "6.504429265055459e-06\n",
      "0.0008480265595915992\n",
      "5.166605763501196e-06\n",
      "0.0008225002725668336\n",
      "6.961716871741755e-06\n",
      "0.0008954330549740517\n",
      "8.797186390215778e-06\n",
      "0.0008001534071813163\n",
      "4.576785862241095e-06\n",
      "0.0007998167470528824\n",
      "4.77582264770351e-06\n",
      "0.0008594389586062114\n",
      "3.3954637100243757e-06\n",
      "0.0008205737998403796\n",
      "4.921246960428431e-06\n",
      "0.0008169469439854471\n",
      "3.16822382083736e-06\n",
      "0.0008009631847690883\n",
      "4.7239413280863234e-06\n",
      "0.0007997032490245764\n",
      "3.2030927430668744e-06\n",
      "0.0008132377147654201\n",
      "9.435691386165438e-06\n",
      "0.0008203224580140571\n",
      "2.7528867325792313e-06\n",
      "0.0008189028486225215\n",
      "4.752288933136246e-06\n",
      "0.0008026107611507222\n",
      "6.557921487279417e-06\n",
      "0.0007893839628433831\n",
      "3.1358304407488157e-06\n",
      "0.000826814522014576\n",
      "7.347736131064779e-06\n",
      "0.0008149880277217237\n",
      "3.81825546980392e-06\n",
      "0.0008172019204015906\n",
      "4.863229780484991e-06\n",
      "0.0007919191951711702\n",
      "6.465128546305498e-06\n",
      "0.000786885088539581\n",
      "1.7595076535213977e-06\n",
      "0.0008316154597953622\n",
      "5.2076317323796785e-06\n",
      "0.000808493404814222\n",
      "1.7370254101902372e-06\n",
      "0.0008944282428574242\n",
      "7.6051214859931065e-06\n",
      "0.0008793278312904509\n",
      "6.021393117118614e-06\n",
      "0.0008277274272092967\n",
      "6.673493250625846e-06\n",
      "0.0009045482660102641\n",
      "6.995970841650074e-06\n",
      "0.0008879762199385496\n",
      "1.0077894223253332e-05\n",
      "0.0008264485123560667\n",
      "5.3867953131416655e-06\n",
      "0.0008314275829898843\n",
      "4.478488869635027e-06\n",
      "0.0007915836018909351\n",
      "3.3629185029166434e-06\n",
      "0.0008407340987465032\n",
      "7.580432973502882e-06\n",
      "0.0008055010171475963\n",
      "8.100724574747104e-06\n",
      "0.0008249604333386389\n",
      "6.4143818955456205e-06\n",
      "0.0007813526811745175\n",
      "1.9893762778403636e-06\n",
      "0.0008780056880931744\n",
      "7.462508907166331e-06\n",
      "0.0008089503671838533\n",
      "3.733991156262739e-06\n",
      "0.0008649082523645243\n",
      "2.3382740120090497e-05\n",
      "0.0008645669871153129\n",
      "6.265018166647696e-06\n",
      "0.0009354603736987129\n",
      "7.4144369835802825e-06\n",
      "0.0009811426108080148\n",
      "1.2098360686919402e-05\n",
      "0.0008613290222658132\n",
      "3.8576607720098865e-06\n",
      "0.000836031095793411\n",
      "5.120090760244797e-06\n",
      "0.0008741937530997707\n",
      "9.285898298186665e-06\n",
      "0.0008745133515695902\n",
      "5.130335000985656e-06\n",
      "0.0008754646200897793\n",
      "3.623991771980972e-06\n",
      "0.0007908198677152841\n",
      "6.05900176290151e-06\n",
      "0.0008251395441345629\n",
      "3.045399809605264e-06\n",
      "0.0008051827068807874\n",
      "4.77596684896269e-06\n",
      "0.0008623620357792357\n",
      "5.679734327719151e-06\n",
      "0.0008139897301904914\n",
      "6.141380606981135e-06\n",
      "0.0008748830787101578\n",
      "5.722503584064219e-06\n",
      "0.0008194808331326212\n",
      "5.06155056207588e-06\n",
      "0.0008500561185379209\n",
      "5.312855401472464e-06\n",
      "0.0008225688046099189\n",
      "3.1213716811383416e-06\n",
      "0.0008390429881587255\n",
      "5.678444537803232e-06\n",
      "0.0008915445116143568\n",
      "5.245747758022095e-06\n",
      "0.0008269607112578888\n",
      "5.157725144196011e-06\n",
      "0.0008368472854355743\n",
      "5.20629783609195e-06\n",
      "0.0009385284801782356\n",
      "9.37949977842909e-06\n",
      "0.0008627242751610785\n",
      "6.5619532911980195e-06\n",
      "0.0009041510661152972\n",
      "3.499291488673115e-05\n",
      "0.000916987952893649\n",
      "6.266720086551953e-06\n",
      "0.0008282940723211263\n",
      "7.197364694964116e-06\n",
      "0.0008658795698336233\n",
      "5.130030589207443e-06\n",
      "0.0008729900610419187\n",
      "9.250841815051589e-06\n",
      "0.0007731592040547957\n",
      "7.51534269660553e-06\n",
      "0.0008751864234159358\n",
      "3.0143525773570097e-06\n",
      "0.0007725686666940651\n",
      "4.625500211460432e-06\n",
      "0.0008118752755576288\n",
      "2.8275703820446752e-06\n",
      "0.0008402802357336177\n",
      "4.08978772046103e-06\n",
      "0.0008216665115779506\n",
      "3.3602174906642442e-06\n",
      "0.0008021231225228236\n",
      "4.006093551779555e-06\n",
      "0.0008606756950985976\n",
      "6.6828245937584404e-06\n",
      "0.0008788473613743587\n",
      "6.978615077613679e-06\n",
      "0.0008157855418263522\n",
      "4.290697910461292e-06\n",
      "0.0008024392733487716\n",
      "4.367615352132396e-06\n",
      "0.0008026730266369092\n",
      "4.610814577853674e-06\n",
      "0.0007927602733846909\n",
      "4.75568649229565e-06\n",
      "0.0007849776667584474\n",
      "4.382085175812838e-06\n",
      "0.0007835000956197099\n",
      "2.9784999342078466e-06\n",
      "0.0007561410821814714\n",
      "2.834772442144838e-06\n",
      "0.0008034359280223572\n",
      "4.5414070208832875e-06\n",
      "0.0007807574153365568\n",
      "5.223740729571179e-06\n",
      "0.0008021360792910519\n",
      "7.943468126499284e-06\n",
      "0.0007943893111316045\n",
      "6.4028601217399474e-06\n",
      "0.0008567232171588296\n",
      "6.231987067785512e-06\n",
      "0.0007985102108907405\n",
      "6.172578501474831e-06\n",
      "0.0008262054735637056\n",
      "6.844046488626105e-06\n",
      "0.0007995513468738047\n",
      "4.386452167031477e-06\n",
      "0.0008219097838486274\n",
      "1.1338224734881834e-05\n",
      "0.000810335352803446\n",
      "2.105279622987633e-06\n",
      "0.0008657543401103248\n",
      "7.059780204281168e-06\n",
      "0.0008249704541729287\n",
      "3.7772472395432416e-06\n",
      "0.000832892707124825\n",
      "1.928535813559754e-05\n",
      "0.0008140511853049489\n",
      "2.593862532066948e-06\n",
      "0.0008384911249454348\n",
      "7.1289139532498685e-06\n",
      "0.0008148091898941032\n",
      "5.079275913705827e-06\n",
      "0.0008472854388388219\n",
      "8.360099749197988e-06\n",
      "0.0008117507107895587\n",
      "7.026929576382742e-06\n",
      "0.0008818601288777874\n",
      "8.662597867383679e-06\n",
      "0.0007995524061996935\n",
      "5.855642136594327e-06\n",
      "0.0008221445150644099\n",
      "3.1005258970999207e-06\n",
      "0.0007926927891613525\n",
      "4.731845446352217e-06\n",
      "0.0008655373201360764\n",
      "2.811284137039795e-05\n",
      "0.0007426392802977357\n",
      "2.625568885217923e-06\n",
      "0.0008997869981874443\n",
      "7.3769797448757635e-06\n",
      "0.0009533997525658332\n",
      "6.64720140512742e-06\n",
      "0.0008614912153172474\n",
      "4.12925331691154e-06\n",
      "0.0008287979188411596\n",
      "2.903038040189912e-06\n",
      "0.0007922923101273141\n",
      "2.6640791500829737e-06\n",
      "0.0008061196072427176\n",
      "3.1460832238929583e-06\n",
      "0.0007689351528642552\n",
      "1.321535919428776e-06\n",
      "0.0007651320031396361\n",
      "6.454697356145886e-06\n",
      "0.0007813735176272831\n",
      "2.4267400308806154e-06\n",
      "0.000825381945440571\n",
      "5.370333288049978e-06\n",
      "0.0007843316454562224\n",
      "3.1538337373018568e-06\n",
      "0.0007715892788063211\n",
      "1.3671393637631296e-06\n",
      "0.0007713353630008042\n",
      "3.6236864141497463e-06\n",
      "0.0008169286181405238\n",
      "2.1490811766101617e-06\n",
      "0.0008215271659638947\n",
      "7.3800505702796826e-06\n",
      "0.0007717607111352888\n",
      "1.4036013212206894e-06\n",
      "0.0008943760861718159\n",
      "3.899092357543494e-06\n",
      "0.0008724444591049475\n",
      "3.895153597059282e-06\n",
      "0.0008517416390126216\n",
      "7.152773136667314e-06\n",
      "0.0008398983307192679\n",
      "2.3068880620650127e-06\n",
      "0.0009354007992473235\n",
      "7.126869077781135e-06\n",
      "0.0008730802986339159\n",
      "3.1751069021058076e-05\n",
      "0.0008245725213303353\n",
      "3.9347763497290875e-06\n",
      "0.0008261748076211687\n",
      "4.455189815114329e-06\n",
      "0.0009823054315629138\n",
      "8.42198329180773e-06\n",
      "0.0008060993516888095\n",
      "3.84823485860973e-06\n",
      "0.0008354006512797646\n",
      "5.224604243028761e-06\n",
      "0.0008159330679424317\n",
      "1.8677784978459368e-06\n",
      "0.0008305113199859705\n",
      "6.35148843642944e-06\n",
      "0.0008859992607753044\n",
      "2.6373637300983853e-06\n",
      "0.0008577739913354657\n",
      "1.6042796972809093e-06\n",
      "0.0007746892544400749\n",
      "4.5121307700134034e-06\n",
      "0.0008992985253035123\n",
      "0.0006008359443166696\n",
      "0.0007828437365781988\n",
      "7.3429800471915616e-06\n",
      "0.0008556807911573637\n",
      "4.519248218557211e-06\n",
      "0.0008796891717274146\n",
      "5.960325992345942e-06\n",
      "0.0008074954259665039\n",
      "6.388562423248058e-06\n",
      "0.0007865513795344762\n",
      "3.1920793691379418e-06\n",
      "0.0007816715793582309\n",
      "8.37797560721383e-06\n",
      "0.0007742744487315656\n",
      "4.3841998094980655e-06\n",
      "0.0007703309201132696\n",
      "3.862132237868065e-06\n",
      "0.0007466317129109422\n",
      "3.2503741868878913e-06\n",
      "0.0007463723773454672\n",
      "4.0485457258869e-06\n",
      "0.0007411505144278481\n",
      "2.530896474098723e-06\n",
      "0.0008074204405873362\n",
      "8.056928571302036e-06\n",
      "0.000787964367658221\n",
      "4.119091333942347e-06\n",
      "0.0008200547643874181\n",
      "4.147684912670437e-06\n",
      "0.0007717967201572933\n",
      "4.497498789789723e-06\n",
      "0.0008510267783774291\n",
      "5.4593859621989175e-06\n",
      "0.0007766171427056222\n",
      "4.977999789834613e-06\n",
      "0.000820798317539649\n",
      "6.926069122219831e-06\n",
      "0.0007906080293801893\n",
      "5.831789599554648e-06\n",
      "0.0008116398313714003\n",
      "4.83168924769163e-06\n",
      "0.0008249423641221462\n",
      "3.1308031107797178e-06\n",
      "0.0009186369740861128\n",
      "6.793320082574471e-06\n",
      "0.0008260203162957755\n",
      "3.6201085585658656e-06\n",
      "0.0008373024382282481\n",
      "6.5461553890858484e-06\n",
      "0.0008266323071711161\n",
      "5.3009519596390275e-06\n",
      "0.0008315547945732287\n",
      "6.635916194979299e-06\n",
      "0.0008560410075067936\n",
      "5.638159966336669e-06\n",
      "0.0008319605723423823\n",
      "5.039051720801849e-06\n",
      "0.0007837544763442252\n",
      "4.78565835390502e-06\n",
      "0.0008354743572035366\n",
      "7.530075410158119e-06\n",
      "0.0007849041578312822\n",
      "4.083199911684659e-06\n",
      "0.0008274809030447271\n",
      "3.6108520879581026e-06\n",
      "0.000789610895954202\n",
      "5.073392683036475e-06\n",
      "0.0008903335257723471\n",
      "7.001995332184065e-06\n",
      "0.0007789671466321153\n",
      "3.068008934717178e-06\n",
      "0.0008341467415594817\n",
      "5.906927880236039e-06\n",
      "0.0008840103242424279\n",
      "7.605047207943168e-06\n",
      "0.0007941541316021829\n",
      "8.495308161051418e-06\n",
      "0.0007980207206716583\n",
      "4.266826200916519e-06\n",
      "0.0007997536500573951\n",
      "3.3927263729061504e-06\n",
      "0.0007671699467710167\n",
      "5.670902698844192e-06\n",
      "0.0007759516076542675\n",
      "5.5287888572047135e-06\n",
      "0.0007966022241565042\n",
      "5.752837086422057e-06\n",
      "0.0007950057862544949\n",
      "2.621973213728967e-06\n",
      "0.0007779986514519062\n",
      "2.084191780491504e-06\n",
      "0.0008057628091213984\n",
      "3.7504887359609032e-06\n",
      "0.0007917266742948538\n",
      "3.930667120116486e-06\n",
      "0.0008177627777498332\n",
      "4.960798465935997e-06\n",
      "0.000808397071259081\n",
      "5.124959183669113e-06\n",
      "0.0008520832427731408\n",
      "4.4470363283048205e-06\n",
      "0.0007838234837097591\n",
      "2.5467580900345345e-06\n",
      "0.0008162837369583713\n",
      "4.830919054901673e-06\n",
      "0.0007990336107611572\n",
      "2.755888799240687e-06\n",
      "0.0008278722560702566\n",
      "7.417031683923837e-06\n",
      "0.0007999237312914322\n",
      "2.802207528058878e-06\n",
      "0.0008686827279933254\n",
      "4.7579933985121475e-06\n",
      "0.0008174651136266159\n",
      "2.0603027051246164e-05\n",
      "0.0008071239633042594\n",
      "2.05833591630092e-05\n",
      "0.0008313110925446002\n",
      "5.054847256919665e-06\n",
      "0.0008301912975391646\n",
      "6.885016272774927e-06\n",
      "0.0007810293205485327\n",
      "4.354032600301894e-06\n",
      "0.0008378858894737757\n",
      "1.204087732427612e-06\n",
      "0.0007807848499479671\n",
      "3.7069654043745064e-06\n",
      "0.0008491343105170373\n",
      "4.137031350733955e-06\n",
      "0.0007825940097945544\n",
      "4.144679514034449e-06\n",
      "0.0008144998487348541\n",
      "1.501570571258484e-06\n",
      "0.0007824979796936341\n",
      "7.086802837616833e-06\n",
      "0.0008603746291464426\n",
      "7.037026158625336e-06\n",
      "0.0007517817650693824\n",
      "4.475140399319668e-06\n",
      "0.0008075285746241604\n",
      "2.3240851590206367e-06\n",
      "0.0008522843551885731\n",
      "7.667160601105558e-06\n",
      "0.0008263708493204211\n",
      "6.185279801311312e-06\n",
      "0.0008062606390915936\n",
      "6.225456492066175e-06\n",
      "0.0008088260377408235\n",
      "5.465185782119842e-06\n",
      "0.0008259448284149474\n",
      "5.131371886333529e-06\n",
      "0.0008073565446454108\n",
      "4.420009645594375e-06\n",
      "0.0008134297422852208\n",
      "3.347939251972372e-06\n",
      "0.0007948933758289605\n",
      "4.1521298179654885e-06\n",
      "0.0007965260775342189\n",
      "8.201345302458403e-06\n",
      "0.0008134147105921726\n",
      "5.863159041157439e-06\n",
      "0.0008012057755499354\n",
      "3.2105216975273236e-06\n",
      "0.0008150698390776195\n",
      "6.005738766096894e-06\n",
      "0.0007993298090361095\n",
      "4.420628960066857e-06\n",
      "0.0008331125705901964\n",
      "2.300760870862018e-06\n",
      "0.0008000495446416611\n",
      "5.322567214843498e-06\n",
      "0.0008903627729000534\n",
      "6.341782611592029e-06\n",
      "0.0009261294903736171\n",
      "9.136927985287606e-06\n",
      "0.0008485972506369707\n",
      "5.333805798411696e-06\n",
      "0.0007979075843656047\n",
      "4.300682321759933e-06\n",
      "0.0008694608464951292\n",
      "8.263054841276636e-06\n",
      "0.0008107164671813896\n",
      "7.80053352944189e-06\n",
      "0.0008436702171373143\n",
      "5.029347199659671e-06\n",
      "0.0008913981188489395\n",
      "3.762525078282017e-06\n",
      "0.0007633520271432033\n",
      "4.432466481877063e-06\n",
      "0.0007710417371690027\n",
      "3.980949426690084e-06\n",
      "0.0008237228643862747\n",
      "5.293611749675553e-06\n",
      "0.000787982957804209\n",
      "4.304500044954067e-06\n",
      "0.0008714161226828991\n",
      "4.976923043517228e-06\n",
      "0.0008040345626047273\n",
      "2.614115490089353e-06\n",
      "0.0008518503603210976\n",
      "7.277217712689985e-06\n",
      "0.0008236783457715823\n",
      "8.324013673774468e-06\n",
      "0.0008732005017696072\n",
      "2.202482729648303e-06\n",
      "0.0008612993646171831\n",
      "8.855336550656715e-06\n",
      "0.0008401376460119155\n",
      "4.928225405969045e-06\n",
      "0.0008789516458461254\n",
      "4.4969823604826875e-06\n",
      "0.0009355404188939008\n",
      "1.103277044394311e-05\n",
      "0.0008909684436874192\n",
      "1.9531708949264806e-05\n",
      "0.0008182265727425486\n",
      "5.940345376385489e-06\n",
      "0.0008314627378803309\n",
      "4.397169119098792e-06\n",
      "0.0008911994575283935\n",
      "9.51874423605133e-06\n",
      "0.00086856953262862\n",
      "5.248629570159451e-06\n",
      "0.0008680445517789032\n",
      "3.7867479537253917e-06\n",
      "0.0008078588989292214\n",
      "6.883997752732014e-06\n",
      "0.0008838843653926877\n",
      "1.2217024126736352e-05\n",
      "0.0007870993001465133\n",
      "6.553361122957025e-06\n",
      "0.0008075688885134391\n",
      "3.763884741675298e-06\n",
      "0.0008350219780040903\n",
      "4.923125296697686e-06\n",
      "0.0009247245291861302\n",
      "7.647185852247645e-06\n",
      "0.0007590430169651721\n",
      "5.396160259281232e-06\n",
      "0.0008209690953257638\n",
      "6.6143271184522705e-06\n",
      "0.0008478382790751773\n",
      "3.0051105215059478e-05\n",
      "0.0008082995376580909\n",
      "5.710901846758394e-06\n",
      "0.0007683067994351021\n",
      "5.858562747808574e-06\n",
      "0.0008186749907956268\n",
      "6.014449562363729e-06\n",
      "0.0007971396505540516\n",
      "6.077364202426801e-06\n",
      "0.0008125533258704072\n",
      "6.334044055588871e-06\n",
      "0.0007576467737785942\n",
      "3.2976425674439e-06\n",
      "0.0007834066980723667\n",
      "6.264607328649279e-06\n",
      "0.0007861708363096002\n",
      "3.520851459899273e-06\n",
      "0.0008293438709393321\n",
      "4.651812879853739e-06\n",
      "0.0008005159603619249\n",
      "5.948855361942344e-06\n",
      "0.0008282875967084536\n",
      "5.991996048660119e-06\n",
      "0.0008090460912374929\n",
      "6.358216820386798e-06\n",
      "0.0008329089819035048\n",
      "4.488675541346588e-06\n",
      "0.0007792886319128999\n",
      "8.298943036185338e-06\n",
      "0.0008370455238462538\n",
      "8.087819037682838e-06\n",
      "0.000821467927244506\n",
      "8.329869609613051e-06\n",
      "0.0008587455088827945\n",
      "7.27834368293474e-06\n",
      "0.0008347272101696248\n",
      "5.033977667293251e-06\n",
      "0.0008773171126845399\n",
      "9.728188969987586e-06\n",
      "0.0008347287328566798\n",
      "3.419009651278073e-06\n",
      "0.0008166489505427761\n",
      "5.521867295208897e-06\n",
      "0.0008442475928762723\n",
      "6.337011113004808e-06\n",
      "0.0008681536078670428\n",
      "1.0747087405832337e-05\n",
      "0.000836980903012002\n",
      "5.798138060009276e-06\n",
      "0.0008441438715785104\n",
      "8.085132135778818e-06\n",
      "0.0008128154573375369\n",
      "7.644092212991942e-06\n",
      "0.0008186431836264134\n",
      "6.792371324239468e-06\n",
      "0.0007836761332896329\n",
      "4.16941317768216e-06\n",
      "0.0008287041792106183\n",
      "3.0079114446850142e-06\n",
      "0.0008523393390003637\n",
      "8.883674734878826e-06\n",
      "0.0008823605188291476\n",
      "8.795072359224083e-06\n",
      "0.0007516419323292754\n",
      "1.6729067632971736e-06\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def find_ith_value(intensity, current, min_slope_fitpnt=1, max_slope_fitpnt=10, window_length=5, polyorder=2):\n",
    "    try:\n",
    "        # Sort data by current to ensure proper processing\n",
    "        sorted_indices = np.argsort(current)\n",
    "        current, intensity = current[sorted_indices], intensity[sorted_indices]\n",
    "\n",
    "        # Normalize intensity using provided max and min PD values\n",
    "        min_intensity, max_intensity = np.min(intensity), np.max(intensity)\n",
    "        intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        # Normalize intensity using min-max scaling\n",
    "        min_intensity = np.min(intensity)\n",
    "        max_intensity = np.max(intensity)\n",
    "        intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        # Apply Savitzky-Golay smoothing to normalized intensity\n",
    "        smoothed_intensity_norm = savgol_filter(intensity_norm, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "        # Compute differentials on normalized & smoothed data\n",
    "        smoothed_dI_dC_norm = np.gradient(smoothed_intensity_norm, current)\n",
    "        smoothed_d2I_dC2_norm = np.gradient(smoothed_dI_dC_norm, current)\n",
    "\n",
    "        # Filter the data to only consider LDI between 2 and 30 mA for Gaussian fitting\n",
    "        mask = (current >= 2) & (current <= 30)\n",
    "        if not np.any(mask):\n",
    "            print(\"Warning: No data points in the 2-30 mA range.\")\n",
    "            return None, None\n",
    "\n",
    "        current_masked = current[mask]\n",
    "        smoothed_d2I_dC2_norm_masked = smoothed_d2I_dC2_norm[mask]\n",
    "\n",
    "        # Fit Gaussian to the smoothed second differential\n",
    "        p0 = [np.max(smoothed_d2I_dC2_norm_masked), np.median(current_masked), np.std(current_masked)]\n",
    "        popt, pcov = curve_fit(gaussian, current_masked, smoothed_d2I_dC2_norm_masked, p0=p0)\n",
    "        median_x = popt[1]  # Extract median x from Gaussian fit\n",
    "\n",
    "        # Handle fitting errors\n",
    "        if not (2 <= median_x <= 30):\n",
    "            print(\"Warning: Gaussian fit unable to find reasonable split point. \")\n",
    "            median_x = 12.5\n",
    "        elif np.any(np.diag(pcov) > 0.1):  # Adjust threshold as needed\n",
    "            print(\"Warning: Abnormal LI curve detected due to high error in Gaussian fit.\")\n",
    "            return None, None\n",
    "\n",
    "        # Split data at median_x\n",
    "        left_side = current[current <= median_x]\n",
    "        right_side_mask = (current > median_x + min_slope_fitpnt) & (\n",
    "            current < median_x + max_slope_fitpnt\n",
    "        )  # Only Fit Right Slope after a few current pnts after ITH, and up to a certain current maximum above ITH.\n",
    "        right_side = current[right_side_mask]\n",
    "        intensity_norm_left = intensity_norm[current <= median_x]\n",
    "        intensity_norm_right = intensity_norm[right_side_mask]\n",
    "\n",
    "        # Check if either side is empty\n",
    "        if len(left_side) == 0 or len(right_side) == 0:\n",
    "            print(\"Warning: No reasonable I_th detected within bounds.\")\n",
    "            return None, None\n",
    "\n",
    "        # Fit linear regression to both segments\n",
    "        slope_left, intercept_left, _, _, _ = linregress(left_side, intensity_norm_left)\n",
    "        slope_right, intercept_right, _, _, _ = linregress(right_side, intensity_norm_right)\n",
    "\n",
    "        # Compute intersection point\n",
    "        intersection_x = (intercept_right - intercept_left) / (slope_left - slope_right)\n",
    "        ith_value = intersection_x  # No rounding\n",
    "\n",
    "        # Final evaluation check for ITH value\n",
    "        if not (2 <= ith_value <= 30):\n",
    "            print(\"Warning: Computed ITH value outside valid bounds (2-30 mA). Returning None.\")\n",
    "            return None, slope_right\n",
    "\n",
    "        return ith_value, slope_right\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Gaussian model for fitting\n",
    "def gaussian(x, a, x0, sigma):\n",
    "    return a * np.exp(-((x - x0) ** 2) / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def find_ith_value_labview(intensity, current):\n",
    "    # try:\n",
    "    # 1) Trim data to only include values >2 and <=35 mA\n",
    "    mask_trimmed = (current > 2) & (current <= 35)\n",
    "    if not np.any(mask_trimmed):\n",
    "        print(\"Warning: No data points between 2 and 35 mA.\")\n",
    "        return 0, 0\n",
    "    current = current[mask_trimmed]\n",
    "    intensity = intensity[mask_trimmed]\n",
    "\n",
    "    # 2) Interpolate to double resolution (spacing of 0.5 mA)\n",
    "    current_interp = np.arange(np.min(current), np.max(current) + 0.1, 0.5)\n",
    "    intensity_interp = np.interp(current_interp, current, intensity)\n",
    "\n",
    "    current = current_interp\n",
    "    intensity = intensity_interp\n",
    "\n",
    "    # 3) Normalize intensity using min-max scaling (first operation)\n",
    "    min_intensity = np.min(intensity)\n",
    "    max_intensity = np.max(intensity)\n",
    "    intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "    # 4) Sort data by current to ensure proper processing\n",
    "    sorted_indices = np.argsort(current)\n",
    "    current = current[sorted_indices]\n",
    "    intensity_norm = intensity_norm[sorted_indices]\n",
    "\n",
    "    # 5) Apply Chebyshev high-pass filter (order 2, ripple 0.1 dB, bandpass 0.15–0.45)\n",
    "    b, a = cheby1(N=2, rp=0.1, Wn=[0.15, 0.45], btype=\"bandpass\", fs=1)\n",
    "    filtered_intensity = filtfilt(b, a, intensity_norm)\n",
    "\n",
    "    # 5a) Initial linear fit via least absolute residuals on filtered data using QuantReg\n",
    "    X = sm.add_constant(current)  # Adds intercept term\n",
    "    model = sm.QuantReg(filtered_intensity, X)\n",
    "    res = model.fit(q=0.5)\n",
    "    slope_left = res.params[1]\n",
    "    intercept_left = res.params[0]\n",
    "    initial_abs_residual_total = np.mean(np.abs(filtered_intensity - res.predict(X)))\n",
    "\n",
    "    print(initial_abs_residual_total)\n",
    "    if initial_abs_residual_total > 0.001:\n",
    "        print(\"Warning: Initial L1 fit residual too high.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # 6) First Savitzky-Golay smoothing (5,1)\n",
    "    smoothed_intensity = savgol_filter(filtered_intensity, window_length=5, polyorder=1)\n",
    "\n",
    "    # 7) Second Savitzky-Golay smoothing before differential (3,2)\n",
    "    smoothed_intensity = savgol_filter(smoothed_intensity, window_length=3, polyorder=2)\n",
    "\n",
    "    # 8) Compute first derivative (renamed to dL_dI)\n",
    "    dL_dI = np.gradient(smoothed_intensity, current)\n",
    "\n",
    "    # 9) Smooth first derivative (6,2)\n",
    "    smoothed_dL_dI = savgol_filter(dL_dI, window_length=6, polyorder=2)\n",
    "\n",
    "    # 10) Compute second derivative (renamed to d2L_dI2)\n",
    "    d2L_dI2 = np.gradient(smoothed_dL_dI, current)\n",
    "\n",
    "    # 11) Smooth second derivative (6,2)\n",
    "    smoothed_d2L_dI2 = savgol_filter(d2L_dI2, window_length=6, polyorder=2)\n",
    "\n",
    "    # 11a) Set negative second derivative values to zero (LabVIEW-like behavior)\n",
    "    smoothed_d2L_dI2[smoothed_d2L_dI2 < 0] = 0\n",
    "\n",
    "    # 12) Normalize second derivative and add 0.01\n",
    "    max_d2L_dI2 = np.max(smoothed_d2L_dI2)\n",
    "    if max_d2L_dI2 == 0:\n",
    "        print(\"Warning: Second derivative all zero after zeroing negatives.\")\n",
    "        return 0, 0\n",
    "    d2L_dI2_ready = (smoothed_d2L_dI2 / max_d2L_dI2) + 0.01\n",
    "\n",
    "    # 13) Least Absolute Residuals fitting with initial conditions and bounds\n",
    "    initial_guess = [1, 11, np.std(current)]\n",
    "    bounds = [\n",
    "        (0, np.inf),  # Bounds for parameter a\n",
    "        (7, np.max(current) - 3),  # Bounds for parameter x0\n",
    "        (0, 3),  # Bounds for parameter sigma\n",
    "    ]\n",
    "    popt, _ = least_absolute_residuals_fit(current, d2L_dI2_ready, gaussian, initial_guess, bounds)\n",
    "    median_x = popt[1]\n",
    "\n",
    "    # 14) Validate split point\n",
    "    if not (2 <= median_x <= 35):\n",
    "        print(\"Warning: Gaussian fit split point out of bounds. Using default 12.5 mA.\")\n",
    "        median_x = 12.5\n",
    "\n",
    "    # 15) Linear fitting on left segment (with gradient bound)\n",
    "    left_mask = current <= median_x\n",
    "    if not np.any(left_mask):\n",
    "        print(\"Warning: No data points on the left segment for fitting.\")\n",
    "        return 0, 0\n",
    "\n",
    "    current_left = current[left_mask]\n",
    "    intensity_left = intensity_norm[left_mask]\n",
    "\n",
    "    popt_left, _ = curve_fit(linear_model, current_left, intensity_left, bounds=([0.001, -np.inf], [np.inf, np.inf]))\n",
    "    slope_left, intercept_left = popt_left\n",
    "\n",
    "    # 16) Linear fitting on right segment (no bounds)\n",
    "    right_mask = current > median_x\n",
    "    if not np.any(right_mask):\n",
    "        print(\"Warning: No data points on the right segment for fitting.\")\n",
    "        return 0, 0\n",
    "\n",
    "    current_right = current[right_mask]\n",
    "    intensity_right = intensity_norm[right_mask]\n",
    "\n",
    "    if len(current_right) < 10:\n",
    "        print(\"Warning: Fewer than 10 data points for stimulated emission fit.\")\n",
    "        return 0, 0\n",
    "\n",
    "    popt_right, _ = curve_fit(linear_model, current_right, intensity_right)\n",
    "    slope_efficiency, intercept_right = popt_right\n",
    "\n",
    "    fitted_right = linear_model(current_right, *popt_right)\n",
    "    mse_right = np.mean((intensity_right - fitted_right) ** 2)\n",
    "\n",
    "    print(mse_right)\n",
    "    if mse_right > 0.001:\n",
    "        print(\"Warning: High MSE in stimulated emission fit.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # 17) Compute intersection (I_th)\n",
    "    ith_value = (intercept_right - intercept_left) / (slope_left - slope_efficiency)\n",
    "    if not (2 <= ith_value <= 35):\n",
    "        print(\"Warning: Computed I_th outside bounds. Returning None.\")\n",
    "        return 0, 0\n",
    "\n",
    "    return ith_value, slope_efficiency\n",
    "\n",
    "\n",
    "def evaluate_ITH_on_rawsweep(df_raw_sweeps, touchdown_number, sampling_rate=1):\n",
    "    df_raw_sweeps[\"ITH\"] = np.nan\n",
    "    for touchdown in range(\n",
    "        sampling_rate, touchdown_number * sampling_rate + 1, sampling_rate\n",
    "    ):  # accounts for selecting every nth touchdown if sampling.\n",
    "        specific_data = df_raw_sweeps[df_raw_sweeps[\"TOUCHDOWN\"] == touchdown]\n",
    "        if not specific_data.empty and \"NO LASER\" not in specific_data[\"FLAG\"].values:\n",
    "            ith_value, slope_efficiency = find_ith_value_labview(\n",
    "                specific_data[\"PD\"].values,\n",
    "                specific_data[\"LDI_mA\"].values,\n",
    "            )\n",
    "            if ith_value != 0:\n",
    "                df_raw_sweeps.loc[df_raw_sweeps[\"TOUCHDOWN\"] == touchdown, \"ITH\"] = ith_value\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "def generate_ITH_device_summary_table(raw_sweeps):\n",
    "\n",
    "    # Create COD Summary table with POT_FAILMODE\n",
    "    device_summary = (\n",
    "        (\n",
    "            raw_sweeps.groupby(\"TE_LABEL\").agg(\n",
    "                {\n",
    "                    \"WAFER_ID\": \"first\",\n",
    "                    \"MACH\": \"first\",\n",
    "                    \"TOUCHDOWN\": \"first\",\n",
    "                    \"ITH\": \"first\",\n",
    "                    \"TYPE\": \"first\",\n",
    "                    \"X_UM\": \"first\",\n",
    "                    \"Y_UM\": \"first\",\n",
    "                    \"FLAG\": \"first\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"TOUCHDOWN\")\n",
    "    )\n",
    "\n",
    "    return device_summary\n",
    "\n",
    "\n",
    "annotated_sweeps_tables = []\n",
    "\n",
    "\n",
    "device_summary_tables = []\n",
    "\n",
    "\n",
    "wafer_summary_tables = []\n",
    "\n",
    "\n",
    "for df_raw_sweeps, num_devices, sampling_rate, wafer_code in zip(\n",
    "    raw_sweeps_tables, device_numbers, sampling_rates, wafer_codes\n",
    "):\n",
    "    # Apply the NO LASER flag function\n",
    "    df_raw_sweeps = flag_no_laser_touchdowns(df_raw_sweeps)\n",
    "    # print(f\"\\nflagged:\\n {df_raw_sweeps}\")\n",
    "\n",
    "    # Run ITH evaluations\n",
    "    df_raw_sweeps = evaluate_ITH_on_rawsweep(df_raw_sweeps, num_devices, sampling_rate)\n",
    "    # print(f\"\\nannotated:\\n {df_raw_sweeps}\")\n",
    "    annotated_sweeps_tables.append(df_raw_sweeps)\n",
    "    # df_raw_sweeps.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_raw_sweeps.csv\", index=False)\n",
    "\n",
    "    # Device Summary\n",
    "    device_summary = generate_ITH_device_summary_table(df_raw_sweeps)\n",
    "    # print(f\"\\ndevice summary:\\n {device_summary}\")\n",
    "    device_summary_tables.append(device_summary)\n",
    "    device_summary.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_device_summary.csv\", index=False)\n",
    "\n",
    "# print(annotated_sweeps_tables[0].head(1000))\n",
    "\n",
    "# print(device_summary_tables[0].head(1000))\n",
    "# print(len(device_summary_tables))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "df_combined = pd.concat(device_summary_tables, ignore_index=True)\n",
    "df_combined.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_device_combined_summary.csv\", index=False)\n",
    "\n",
    "# Display the first 10 rows of the combined dataframe\n",
    "# print(df_combined.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Sweep Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annotated_sweeps_tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Find the correct dataframe where the wafer code matches the input\u001b[39;00m\n\u001b[32m    202\u001b[39m df_raw_sweeps = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mannotated_sweeps_tables\u001b[49m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mWAFER_ID\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m] == WAFER_CODE:\n\u001b[32m    205\u001b[39m         df_raw_sweeps = df\n",
      "\u001b[31mNameError\u001b[39m: name 'annotated_sweeps_tables' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to plot PD/LDI and Vf/LDI for a specific laser and wafer\n",
    "def plot_specific_touchdown(df_raw_sweeps, wafer_code, touchdown, pnt_size):\n",
    "    specific_data = df_raw_sweeps[(df_raw_sweeps[\"WAFER_ID\"] == wafer_code) & (df_raw_sweeps[\"TOUCHDOWN\"] == touchdown)]\n",
    "\n",
    "    if specific_data.empty:\n",
    "        print(f\"No data found for Wafer Code: {wafer_code} and TOUCHDOWN: {touchdown}\")\n",
    "        return\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "    # Plot PD/LDI\n",
    "    ax1.scatter(specific_data[\"LDI_mA\"], specific_data[\"PD\"], s=pnt_size, color=\"blue\")\n",
    "    ax1.set_title(f\"{wafer_code}: Scatter Plot of PD vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax1.set_xlabel(\"LDI_mA\")\n",
    "    ax1.set_ylabel(\"PD\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot dP/dI\n",
    "    ax3.scatter(specific_data[\"LDI_mA\"], specific_data[\"dP/dI\"], s=pnt_size, color=\"blue\")\n",
    "    ax3.set_title(f\"{wafer_code}: Scatter Plot of dP/dI for TOUCHDOWN {touchdown}\")\n",
    "    ax3.set_xlabel(\"LDI_mA\")\n",
    "    ax3.set_ylabel(\"dP/dI\")\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plot d2P/dI2\n",
    "    ax5.scatter(specific_data[\"LDI_mA\"], specific_data[\"d2P/dI2\"], s=pnt_size, color=\"blue\")\n",
    "    ax5.set_title(f\"{wafer_code}: Scatter Plot of d2P/dI2 for TOUCHDOWN {touchdown}\")\n",
    "    ax5.set_xlabel(\"LDI_mA\")\n",
    "    ax5.set_ylabel(\"d2P/dI2\")\n",
    "    ax5.grid(True)\n",
    "\n",
    "    # Plot Vf/LDI\n",
    "    ax2.scatter(specific_data[\"LDI_mA\"], specific_data[\"Vf\"], s=pnt_size, color=\"green\")\n",
    "    ax2.set_title(f\"{wafer_code}: Scatter Plot of Vf vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax2.set_xlabel(\"LDI_mA\")\n",
    "    ax2.set_ylabel(\"Vf\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot dV/dI\n",
    "    ax4.scatter(specific_data[\"LDI_mA\"], specific_data[\"dV/dI\"], s=pnt_size, color=\"green\")\n",
    "    ax4.set_title(f\"{wafer_code}: Scatter Plot of dV/dI vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax4.set_xlabel(\"LDI_mA\")\n",
    "    ax4.set_ylabel(\"dV/dI\")\n",
    "    ax4.grid(True)\n",
    "\n",
    "    # Plot d2V/dI2\n",
    "    ax6.scatter(specific_data[\"LDI_mA\"], specific_data[\"d2V/dI2\"], s=pnt_size, color=\"green\")\n",
    "    ax6.set_title(f\"{wafer_code}: Scatter Plot of d2V/dI2 vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax6.set_xlabel(\"LDI_mA\")\n",
    "    ax6.set_ylabel(\"d2V/dI2\")\n",
    "    ax6.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gaussian(x, a, mu, sigma):\n",
    "    \"\"\"Gaussian function for curve fitting.\"\"\"\n",
    "    return a * np.exp(-((x - mu) ** 2) / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def plot_with_smoothing_and_normalization(\n",
    "    df_raw_sweeps, wafer_code, touchdown, window_length=5, polyorder=2, max_slope_fitpnt=20\n",
    "):\n",
    "    specific_data = df_raw_sweeps[(df_raw_sweeps[\"WAFER_ID\"] == wafer_code) & (df_raw_sweeps[\"TOUCHDOWN\"] == touchdown)]\n",
    "\n",
    "    if specific_data.empty:\n",
    "        print(f\"No data found for Wafer Code: {wafer_code} and TOUCHDOWN: {touchdown}\")\n",
    "        return\n",
    "\n",
    "    # Sort data by LDI to ensure proper plotting\n",
    "    specific_data = specific_data.sort_values(by=\"LDI_mA\")\n",
    "\n",
    "    # Normalize PD using min-max scaling\n",
    "    min_PD = specific_data[\"PD\"].min()\n",
    "    max_PD = specific_data[\"PD\"].max()\n",
    "    specific_data[\"PD_norm\"] = (specific_data[\"PD\"] - min_PD) / (max_PD - min_PD)\n",
    "\n",
    "    # Apply Savitzky-Golay smoothing to normalized PD\n",
    "    smoothed_PD_norm = savgol_filter(specific_data[\"PD_norm\"], window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "    # Compute differentials on normalized & smoothed data\n",
    "    dP_dI_norm = np.gradient(specific_data[\"PD_norm\"], specific_data[\"LDI_mA\"])\n",
    "    smoothed_dP_dI_norm = np.gradient(smoothed_PD_norm, specific_data[\"LDI_mA\"])\n",
    "\n",
    "    d2P_dI2_norm = np.gradient(dP_dI_norm, specific_data[\"LDI_mA\"])\n",
    "    smoothed_d2P_dI2_norm = np.gradient(smoothed_dP_dI_norm, specific_data[\"LDI_mA\"])\n",
    "\n",
    "    # Fit Gaussian to the smoothed second differential\n",
    "    try:\n",
    "        mask = (specific_data[\"LDI_mA\"] >= 2) & (specific_data[\"LDI_mA\"] <= 30)\n",
    "        current_masked = specific_data[\"LDI_mA\"][mask]\n",
    "        smoothed_d2P_dI2_norm_masked = smoothed_d2P_dI2_norm[mask]\n",
    "\n",
    "        p0 = [np.max(smoothed_d2P_dI2_norm_masked), np.median(current_masked), np.std(current_masked)]\n",
    "        popt, _ = curve_fit(gaussian, current_masked, smoothed_d2P_dI2_norm_masked, p0=p0)\n",
    "        gaussian_fit = gaussian(specific_data[\"LDI_mA\"], *popt)\n",
    "        median_x = popt[1]\n",
    "    except:\n",
    "        median_x = np.nan\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 20))  # Extra row for line fitting plot\n",
    "\n",
    "    # Normalized PD vs LDI (Unsmoothed)\n",
    "    axes[0, 0].plot(specific_data[\"LDI_mA\"], specific_data[\"PD_norm\"], color=\"blue\", label=\"Normalized PD (Raw)\")\n",
    "    axes[0, 0].set_title(f\"{wafer_code}: PD vs LDI (Normalized, Unsmoothed)\")\n",
    "    axes[0, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[0, 0].set_ylabel(\"Normalized PD\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Normalized PD vs LDI (Smoothed)\n",
    "    axes[0, 1].plot(specific_data[\"LDI_mA\"], smoothed_PD_norm, color=\"red\", label=\"Normalized PD (Smoothed)\")\n",
    "    axes[0, 1].set_title(f\"{wafer_code}: PD vs LDI (Normalized & Smoothed)\")\n",
    "    axes[0, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[0, 1].set_ylabel(\"Normalized PD\")\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Normalized dP/dI (Unsmoothed)\n",
    "    axes[1, 0].plot(specific_data[\"LDI_mA\"], dP_dI_norm, color=\"blue\", label=\"dP/dI (Raw)\")\n",
    "    axes[1, 0].set_title(f\"{wafer_code}: dP/dI (Normalized, Unsmoothed)\")\n",
    "    axes[1, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[1, 0].set_ylabel(\"dP/dI\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Normalized dP/dI (Smoothed)\n",
    "    axes[1, 1].plot(specific_data[\"LDI_mA\"], smoothed_dP_dI_norm, color=\"red\", label=\"dP/dI (Smoothed)\")\n",
    "    axes[1, 1].set_title(f\"{wafer_code}: dP/dI (Normalized & Smoothed)\")\n",
    "    axes[1, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[1, 1].set_ylabel(\"dP/dI\")\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # Normalized d2P/dI2 (Unsmoothed)\n",
    "    axes[2, 0].plot(specific_data[\"LDI_mA\"], d2P_dI2_norm, color=\"blue\", label=\"d2P/dI2 (Raw)\")\n",
    "    axes[2, 0].set_title(f\"{wafer_code}: d2P/dI2 (Normalized, Unsmoothed)\")\n",
    "    axes[2, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[2, 0].set_ylabel(\"d2P/dI2\")\n",
    "    axes[2, 0].grid(True)\n",
    "\n",
    "    # Normalized d2P/dI2 (Smoothed) + Gaussian Fit\n",
    "    axes[2, 1].plot(specific_data[\"LDI_mA\"], smoothed_d2P_dI2_norm, color=\"red\", label=\"d2P/dI2 (Smoothed)\")\n",
    "    axes[2, 1].plot(specific_data[\"LDI_mA\"], gaussian_fit, color=\"purple\", linestyle=\"dashed\", label=\"Gaussian Fit\")\n",
    "\n",
    "    # Mark median_x with a vertical line\n",
    "    if not np.isnan(median_x):\n",
    "        axes[2, 1].axvline(median_x, color=\"black\", linestyle=\"dotted\", label=f\"Median: {median_x:.3f}\")\n",
    "\n",
    "    axes[2, 1].set_title(f\"{wafer_code}: d2P/dI2 (Normalized & Smoothed) + Gaussian Fit\")\n",
    "    axes[2, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[2, 1].set_ylabel(\"d2P/dI2\")\n",
    "    axes[2, 1].grid(True)\n",
    "    axes[2, 1].legend()\n",
    "\n",
    "    # ------------------- LINEAR FITTING SECTION (Separate Figure) -------------------\n",
    "    if not np.isnan(median_x):\n",
    "        # Split data at median_x\n",
    "        left_side = specific_data[specific_data[\"LDI_mA\"] <= median_x]\n",
    "        right_side_mask = (specific_data[\"LDI_mA\"] > median_x) & (specific_data[\"LDI_mA\"] < max_slope_fitpnt)\n",
    "        right_side = specific_data[right_side_mask]\n",
    "\n",
    "        if not left_side.empty and not right_side.empty:\n",
    "            # Fit linear regression to both segments\n",
    "            slope_left, intercept_left, _, _, _ = linregress(left_side[\"LDI_mA\"], left_side[\"PD_norm\"])\n",
    "            slope_right, intercept_right, _, _, _ = linregress(right_side[\"LDI_mA\"], right_side[\"PD_norm\"])\n",
    "\n",
    "            # Generate fitted lines\n",
    "            fit_left = slope_left * left_side[\"LDI_mA\"] + intercept_left\n",
    "            fit_right = slope_right * right_side[\"LDI_mA\"] + intercept_right\n",
    "\n",
    "            # Compute intersection point\n",
    "            intersection_x = (intercept_right - intercept_left) / (slope_left - slope_right)\n",
    "            intersection_y = slope_left * intersection_x + intercept_left\n",
    "\n",
    "        # Create a separate figure for linear fits\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot original normalized PD data as scatter points\n",
    "        plt.scatter(\n",
    "            specific_data[\"LDI_mA\"], specific_data[\"PD_norm\"], color=\"green\", marker=\"+\", alpha=1, label=\"Original Data\"\n",
    "        )\n",
    "\n",
    "        # Plot linear fits\n",
    "        plt.plot(left_side[\"LDI_mA\"], fit_left, color=\"blue\", label=f\"Left Fit (Slope={slope_left:.3f})\")\n",
    "        plt.plot(right_side[\"LDI_mA\"], fit_right, color=\"red\", label=f\"Right Fit (Slope={slope_right:.3f})\")\n",
    "\n",
    "        # Mark the intersection point\n",
    "        plt.scatter(\n",
    "            intersection_x,\n",
    "            intersection_y,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            s=200,\n",
    "            label=f\"Intersection at LDI={intersection_x:.3f}\",\n",
    "        )\n",
    "\n",
    "        # Labels and title\n",
    "        plt.title(f\"{wafer_code}: Linear Fits Split at Median\")\n",
    "        plt.xlabel(\"LDI_mA\")\n",
    "        plt.ylabel(\"Normalized PD\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# INPUT THE DESIRED PROFILE TO EXAMINE HERE\n",
    "# Define the specific wafer code and TOUCHDOWN number\n",
    "WAFER_CODE = \"QCHZZ\"\n",
    "TOUCHDOWN = 20\n",
    "\n",
    "# Find the correct dataframe where the wafer code matches the input\n",
    "df_raw_sweeps = None\n",
    "for df in annotated_sweeps_tables:\n",
    "    if df[\"WAFER_ID\"].iloc[0] == WAFER_CODE:\n",
    "        df_raw_sweeps = df\n",
    "        break\n",
    "\n",
    "if df_raw_sweeps is not None:\n",
    "    # Plot for the specified touchdown number\n",
    "    # plot_specific_touchdown(df_raw_sweeps, WAFER_CODE, TOUCHDOWN, pnt_size=5)\n",
    "    plot_with_smoothing_and_normalization(df_raw_sweeps, WAFER_CODE, TOUCHDOWN, window_length=5, polyorder=2)\n",
    "    ITH_value = find_ith_value(df_raw_sweeps, \"QCHZZ\", TOUCHDOWN)\n",
    "    print(f\"ITH value: {ITH_value}\")\n",
    "else:\n",
    "    print(f\"No data found for Wafer Code: {WAFER_CODE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
