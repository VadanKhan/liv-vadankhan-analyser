{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import cheby1, filtfilt, savgol_filter\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "\n",
    "\n",
    "# Move to the root directory\n",
    "\n",
    "\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "\n",
    "# Add the root directory to the system path\n",
    "\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "\n",
    "# Import the importlib module\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "# import function implementations\n",
    "import stst_urls\n",
    "\n",
    "\n",
    "# Reload the modules\n",
    "\n",
    "\n",
    "importlib.reload(stst_urls)\n",
    "\n",
    "\n",
    "# Re-import the functions\n",
    "\n",
    "\n",
    "from stst_urls import GTX_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileserver link: https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/\n",
      "['https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI2K/LIV_53_QCI2K_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI2X/LIV_53_QCI2X_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv', 'https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI12/LIV_53_QCI12_DNS-LIVTKCOD_LIVTK-DNS_STX_processed.csv']\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\n",
    "    \"QCI12\",\n",
    "    \"QCI2X\",\n",
    "    \"QCI2K\",\n",
    "]  # List of wafer codes\n",
    "\n",
    "# Recent Good Wafers\n",
    "\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"Mechanistic_Port_Study\"\n",
    "\n",
    "DECODER_FILE = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "DECODER_FILE_PATH = ROOT_DIR / \"decoders\" / DECODER_FILE\n",
    "RESULTS_FILE_PATH = ROOT_DIR / \"results\"\n",
    "\n",
    "EXPORTS_FILEPATH = ROOT_DIR / \"exports\"\n",
    "# Create the exports folder if it doesn't exist\n",
    "if not os.path.exists(EXPORTS_FILEPATH):\n",
    "    os.makedirs(EXPORTS_FILEPATH)\n",
    "# print(EXPORTS_FILEPATH)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def liv_raw_filelink_finder(wafer_codes, fileserver_link: str, product_code=\"QC\"):\n",
    "    fileserver_link = f\"{fileserver_link}{product_code}/\"\n",
    "    print(f\"fileserver link: {fileserver_link}\")\n",
    "\n",
    "    response = requests.get(fileserver_link, verify=False)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    subdirectory_urls = []\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        if href and any(wafer_code in href for wafer_code in wafer_codes):\n",
    "            subdirectory_urls.append(fileserver_link + href)\n",
    "\n",
    "    # RAW file tracking\n",
    "    file_urls = []\n",
    "    file_cod_urls = []\n",
    "    file_degradation_urls = []\n",
    "    file_times = []\n",
    "    file_cod_times = []\n",
    "    file_degradation_times = []\n",
    "\n",
    "    machine_list = []\n",
    "    machine_dict = {}\n",
    "\n",
    "    # Processed COD tracking\n",
    "    processed_cod70_urls = []\n",
    "    processed_cod250_urls = []\n",
    "    processed_cod_base_urls = []\n",
    "\n",
    "    for wafer_code, subdirectory_url in zip(wafer_codes, subdirectory_urls):\n",
    "        response = requests.get(subdirectory_url, verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "\n",
    "        latest_file = None\n",
    "        latest_cod_file = None\n",
    "        latest_degradation_file = None\n",
    "        latest_time = \"\"\n",
    "        latest_cod_time = \"\"\n",
    "        latest_degradation_time = \"\"\n",
    "        machine_name = None\n",
    "\n",
    "        # Processed COD placeholders\n",
    "        proc_cod70 = None\n",
    "        proc_cod250 = None\n",
    "        proc_cod_base = None\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "\n",
    "            # RAW file logic\n",
    "            if \"RAW\" in href:\n",
    "                time_str = href[-18:-4]  # Timestamp from filename\n",
    "\n",
    "                if not machine_name:\n",
    "                    machine_name = href[:6]  # Extract machine name\n",
    "\n",
    "                if \"COD250\" in href:\n",
    "                    if time_str > latest_cod_time:\n",
    "                        latest_cod_time = time_str\n",
    "                        latest_cod_file = subdirectory_url + href\n",
    "                elif \"COD70\" in href:\n",
    "                    if time_str > latest_degradation_time:\n",
    "                        latest_degradation_time = time_str\n",
    "                        latest_degradation_file = subdirectory_url + href\n",
    "                else:\n",
    "                    if time_str > latest_time:\n",
    "                        latest_time = time_str\n",
    "                        latest_file = subdirectory_url + href\n",
    "\n",
    "            # Processed COD logic (only pick the first match for each type)\n",
    "            elif \"processed\" in href and \"COD\" in href:\n",
    "                full_url = subdirectory_url + href\n",
    "                if \"COD250\" in href and proc_cod250 is None:\n",
    "                    proc_cod250 = full_url\n",
    "                elif \"COD70\" in href and proc_cod70 is None:\n",
    "                    proc_cod70 = full_url\n",
    "                elif \"COD\" in href and \"COD250\" not in href and \"COD70\" not in href and proc_cod_base is None:\n",
    "                    proc_cod_base = full_url\n",
    "\n",
    "        # Append results\n",
    "        if latest_file:\n",
    "            file_urls.append(latest_file)\n",
    "            file_times.append(latest_time)\n",
    "        if latest_cod_file:\n",
    "            file_cod_urls.append(latest_cod_file)\n",
    "            file_cod_times.append(latest_cod_time)\n",
    "        if latest_degradation_file:\n",
    "            file_degradation_urls.append(latest_degradation_file)\n",
    "            file_degradation_times.append(latest_degradation_time)\n",
    "        if machine_name:\n",
    "            machine_list.append(machine_name)\n",
    "            machine_dict[wafer_code] = machine_name\n",
    "\n",
    "        # Append processed CODs\n",
    "        processed_cod70_urls.append(proc_cod70)\n",
    "        processed_cod250_urls.append(proc_cod250)\n",
    "        processed_cod_base_urls.append(proc_cod_base)\n",
    "\n",
    "    return (\n",
    "        file_urls,\n",
    "        file_cod_urls,\n",
    "        file_degradation_urls,\n",
    "        file_times,\n",
    "        file_cod_times,\n",
    "        file_degradation_times,\n",
    "        machine_list,\n",
    "        machine_dict,\n",
    "        processed_cod70_urls,\n",
    "        processed_cod250_urls,\n",
    "        processed_cod_base_urls,\n",
    "    )\n",
    "\n",
    "\n",
    "# Calling code\n",
    "(\n",
    "    file_urls,\n",
    "    file_cod_urls,\n",
    "    file_degradation_urls,\n",
    "    file_times,\n",
    "    file_cod_times,\n",
    "    file_degradation_times,\n",
    "    machine_list,\n",
    "    machine_dict,\n",
    "    processed_cod70_urls,\n",
    "    processed_cod250_urls,\n",
    "    processed_cod_base_urls,\n",
    ") = liv_raw_filelink_finder(wafer_codes, GTX_URL, \"QC\")\n",
    "print(processed_cod_base_urls)\n",
    "\n",
    "# DEBUG: INPUT LINKS TO OTHER GTX FILES HERE\n",
    "# file_urls = [\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_DNS-LIVTKCOD_LCRVCOD250-DNS_RAW20250227044906.CSV\",\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_LIVBLTKCOD_COD250-DNS_RAW20250228082707.CSV\",\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_LIVBLTKCOD_COD250-DNS_RAW20250311164324.CSV\",\n",
    "# ]\n",
    "# print(file_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data to Desired Raw Sweep Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selects required columns\n",
    "- transposes\n",
    "- stacks data in tall format\n",
    "- adds in device coords from decoder file\n",
    "- loops for every csv file chosen, and stores raw_sweep dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 9.59 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.00 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.00 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.01 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.20 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 10.82 seconds\n",
      "     MACH WAFER_ID        Vf  TOUCHDOWN        PD  X_UM   Y_UM TE_LABEL  \\\n",
      "0  LIV_53    QCI12  1.371842      10000  0.009079  5134 -55832    67H2U   \n",
      "1  LIV_53    QCI12  1.434960      10000  0.012127  5134 -55832    67H2U   \n",
      "2  LIV_53    QCI12  1.472011      10000  0.012127  5134 -55832    67H2U   \n",
      "3  LIV_53    QCI12  1.495700      10000  0.013651  5134 -55832    67H2U   \n",
      "4  LIV_53    QCI12  1.514097      10000  0.016699  5134 -55832    67H2U   \n",
      "5  LIV_53    QCI12  1.529792      10000  0.019748  5134 -55832    67H2U   \n",
      "6  LIV_53    QCI12  1.543749      10000  0.019748  5134 -55832    67H2U   \n",
      "7  LIV_53    QCI12  1.556508      10000  0.019748  5134 -55832    67H2U   \n",
      "8  LIV_53    QCI12  1.568380      10000  0.021272  5134 -55832    67H2U   \n",
      "9  LIV_53    QCI12  1.579517      10000  0.025844  5134 -55832    67H2U   \n",
      "\n",
      "       TYPE  LDI_mA  \n",
      "0  BL LASER       1  \n",
      "1  BL LASER       2  \n",
      "2  BL LASER       3  \n",
      "3  BL LASER       4  \n",
      "4  BL LASER       5  \n",
      "5  BL LASER       6  \n",
      "6  BL LASER       7  \n",
      "7  BL LASER       8  \n",
      "8  BL LASER       9  \n",
      "9  BL LASER      10  \n"
     ]
    }
   ],
   "source": [
    "def transform_raw_liv_file(file_url, decoder_file_path, machine_code, wafer_id):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the CSV file from the URL, skipping the first 19 rows\n",
    "    print(\"Step 1: Reading the CSV file...\")\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        skiprows=19,\n",
    "    )\n",
    "    print(f\"Step 1 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 3: Subsetting the data frame...\")\n",
    "    col_names = df.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Transpose the data frame and reset index\n",
    "    print(\"Step 4: Transposing the data frame...\")\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 5: Splitting the transposed table...\")\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Learn data dimensions\n",
    "    print(\"Step 6: Learning data dimensions...\")\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Concatenate all Voltage columns into one\n",
    "    print(\"Step 7: Concatenating Voltage columns...\")\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = [i // n_meas + 1 for i in range(n_meas * n_devices)]\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all PD columns into one\n",
    "    print(\"Step 8: Concatenating PD columns...\")\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 9: Performing Cartesian join...\")\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Add device coordinates from original RAW file\n",
    "    print(\"Step 10: Adding device coordinates...\")\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(\n",
    "            df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 11: Merging with decoder file...\")\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Rename the columns\n",
    "    print(\"Step 12: Renaming columns...\")\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 13: Adding current column...\")\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 14: Adding WAFER_ID column...\")\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 15: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "def transform_raw_liv_file_first_n_rows(file_url, decoder_file_path, machine_code, wafer_id, n):\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    # Step 2: Read the data rows, skipping the header rows\n",
    "    print(\"Step 2: Reading the data rows, skipping the header rows...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_url, skiprows=19, nrows=n)\n",
    "    print(f\"Step 2 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 3: Subsetting the data frame...\")\n",
    "    start_time = time.time()\n",
    "    col_names = df.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Transpose the data frame and reset index\n",
    "    print(\"Step 4: Transposing the data frame...\")\n",
    "    start_time = time.time()\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 5: Splitting the transposed table...\")\n",
    "    start_time = time.time()\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Learn data dimensions\n",
    "    print(\"Step 6: Learning data dimensions...\")\n",
    "    start_time = time.time()\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Concatenate all Voltage columns into one\n",
    "    print(\"Step 7: Concatenating Voltage columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = [i // n_meas + 1 for i in range(n_meas * n_devices)]\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all PD columns into one\n",
    "    print(\"Step 8: Concatenating PD columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 9: Performing Cartesian join...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Add device coordinates from original RAW file\n",
    "    print(\"Step 10: Adding device coordinates...\")\n",
    "    start_time = time.time()\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(\n",
    "            df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 11: Merging with decoder file...\")\n",
    "    start_time = time.time()\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Rename the columns\n",
    "    print(\"Step 12: Renaming columns...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 13: Adding current column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 14: Adding WAFER_ID column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 15: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time_overall\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    sampling_rate = 1\n",
    "\n",
    "    return (df_raw_sweeps, n_meas, n_devices, sampling_rate)\n",
    "\n",
    "\n",
    "def transform_raw_liv_file_every_nth_laser(file_url, decoder_file_path, machine_code, wafer_id, n=10000):\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    # Step 2: Read the data rows, skipping the header rows\n",
    "    print(\"Step 2: Reading the data rows, skipping the header rows...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_url, skiprows=19)\n",
    "    print(f\"Step 2 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Filter every nth laser based on TOUCHDOWN\n",
    "    print(f\"Step 3: Filtering every {n}th laser...\")\n",
    "    start_time = time.time()\n",
    "    df_filtered = df[df[\"TOUCHDOWN\"] % n == 0]\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 4: Subsetting the data frame...\")\n",
    "    start_time = time.time()\n",
    "    col_names = df_filtered.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df_filtered[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Transpose the data frame and reset index\n",
    "    print(\"Step 5: Transposing the data frame...\")\n",
    "    start_time = time.time()\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 6: Splitting the transposed table...\")\n",
    "    start_time = time.time()\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Learn data dimensions\n",
    "    print(\"Step 7: Learning data dimensions...\")\n",
    "    start_time = time.time()\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all Voltage columns into one\n",
    "    print(\"Step 8: Concatenating Voltage columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    # Instead of generating a new TOUCHDOWN, reuse the one from df_filtered\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = df_filtered[\"TOUCHDOWN\"].repeat(n_meas).values\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Concatenate all PD columns into one\n",
    "    print(\"Step 9: Concatenating PD columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 10: Performing Cartesian join...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Add device coordinates from original RAW file\n",
    "    print(\"Step 11: Adding device coordinates...\")\n",
    "    start_time = time.time()\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(\n",
    "            df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 12: Merging with decoder file...\")\n",
    "    start_time = time.time()\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Rename the columns\n",
    "    print(\"Step 13: Renaming columns...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 14: Adding current column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 15: Adding WAFER_ID column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 16: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 16: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 16 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time_overall\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    sampling_rate = n\n",
    "\n",
    "    return (df_raw_sweeps, n_meas, n_devices, sampling_rate)\n",
    "\n",
    "\n",
    "raw_sweeps_tables = []\n",
    "device_numbers = []\n",
    "sampling_rates = []\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROW_NUMBER = 1000\n",
    "\n",
    "# # CAL\n",
    "# for file_url, machine_code in zip(file_urls, machine_list):\n",
    "#     df_raw_sweeps, n_meas, n_devices = transform_raw_liv_file_first_n_rows(file_url, DECODER_FILE_PATH, machine_code, ROW_NUMBER)\n",
    "#     if df_raw_sweeps[\"TE_LABEL\"].isna().any():\n",
    "#         raise ValueError(\"ERROR: Decoder Matching Failed! Perhaps the wrong decoder file was used, no matching X or Y coords found\")\n",
    "#     raw_sweeps_tables.append(df_raw_sweeps)\n",
    "#     device_numbers.append(n_devices)\n",
    "\n",
    "# # Display the first 10 rows of the raw_sweeps table\n",
    "# print(raw_sweeps_tables[0].head(10))\n",
    "# print(device_numbers[0])\n",
    "\n",
    "# DEBUG: CALLING SAMPLED DATA ACROSS MULTIPLE WAFERS\n",
    "SAMPLE_ROWS = 10000  # You can change this value to any other number as needed\n",
    "for file_url, machine_code, wafer_code in zip(file_urls, machine_list, wafer_codes):\n",
    "    df_raw_sweeps, n_meas, n_devices, sampling_rate = transform_raw_liv_file_every_nth_laser(\n",
    "        file_url, DECODER_FILE_PATH, machine_code, wafer_code, n=SAMPLE_ROWS\n",
    "    )\n",
    "    if df_raw_sweeps[\"TE_LABEL\"].isna().any():\n",
    "        raise ValueError(\n",
    "            \"ERROR: Decoder Matching Failed! Perhaps the wrong decoder file was used, no matching X or Y coords found\"\n",
    "        )\n",
    "    raw_sweeps_tables.append(df_raw_sweeps)\n",
    "    device_numbers.append(n_devices)\n",
    "    sampling_rates.append(sampling_rate)\n",
    "\n",
    "# # Concatenate all dataframes together\n",
    "# df_combined = pd.concat(raw_sweeps_tables, ignore_index=True)\n",
    "# # easy measure to make rest of code call:\n",
    "# raw_sweeps_tables = [df_combined]\n",
    "\n",
    "# # Display the first 10 rows of the combined dataframe\n",
    "# print(df_combined.head(10))\n",
    "# print(raw_sweeps_tables[0].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_sweep_analysis(df):\n",
    "    \"\"\"\n",
    "    Compute first and second order differentials for voltage (Vf) and photodiode signal (PD)\n",
    "    while ensuring calculations remain per device.\n",
    "    Additionally, compute min and max PD per touchdown and clone max PD across the sweep.\n",
    "    \"\"\"\n",
    "    df[\"dV/dI\"] = df.groupby(\"TOUCHDOWN\")[\"Vf\"].diff()\n",
    "    df[\"dP/dI\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].diff()\n",
    "    df[\"d2V/dI2\"] = df.groupby(\"TOUCHDOWN\")[\"dV/dI\"].diff()\n",
    "    df[\"d2P/dI2\"] = df.groupby(\"TOUCHDOWN\")[\"dP/dI\"].diff()\n",
    "\n",
    "    df[\"MAX_PD\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].transform(\"max\")\n",
    "    df[\"MIN_PD\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].transform(\"min\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def flag_no_laser_touchdowns(df_raw_sweeps):\n",
    "    \"\"\"\n",
    "    Adds a \"FLAG\" column to df_raw_sweeps, labeling touchdowns as \"NO LASER\"\n",
    "    if the max PD value for that touchdown is below 1.\n",
    "    \"\"\"\n",
    "    df_raw_sweeps[\"FLAG\"] = np.nan\n",
    "    no_laser_touchdowns = df_raw_sweeps.groupby(\"TOUCHDOWN\")[\"PD\"].max()\n",
    "    no_laser_touchdowns = no_laser_touchdowns[no_laser_touchdowns < 1].index\n",
    "    df_raw_sweeps.loc[df_raw_sweeps[\"TOUCHDOWN\"].isin(no_laser_touchdowns), \"FLAG\"] = \"NO LASER\"\n",
    "    return df_raw_sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I_th Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device summary:\n",
      "    TE_LABEL WAFER_ID    MACH  TOUCHDOWN        ITH      TYPE   X_UM   Y_UM  \\\n",
      "29    67H2U    QCI12  LIV_53      10000  12.586994  BL LASER   5134 -55832   \n",
      "30    70J55    QCI12  LIV_53      20000  13.282264  BL LASER  35677 -51599   \n",
      "32    84C0I    QCI12  LIV_53      30000  12.403303  BL LASER -12576 -46532   \n",
      "31    83F2U    QCI12  LIV_53      40000  12.489741  BL LASER -25666 -42332   \n",
      "33    87C5H    QCI12  LIV_53      50000  12.446522  BL LASER  19507 -37499   \n",
      "2    104E0I    QCI12  LIV_53      60000  12.309962  BL LASER  -3336 -33032   \n",
      "3    105D26    QCI12  LIV_53      70000  12.283566  BL LASER   8214 -30032   \n",
      "0    103A3M    QCI12  LIV_53      80000  12.330304  BL LASER -11549 -27266   \n",
      "4    106J5G    QCI12  LIV_53      90000  12.389249  BL LASER  13861 -23966   \n",
      "1    103K74    QCI12  LIV_53     100000  12.358727  BL LASER -19249 -20966   \n",
      "8    126K1H    QCI12  LIV_53     110000  12.480261  BL LASER  24127 -17699   \n",
      "6    121K36    QCI12  LIV_53     120000  12.218625  BL LASER -29516 -14732   \n",
      "7    126D4U    QCI12  LIV_53     130000  12.500332  BL LASER  29774 -11732   \n",
      "5    120D6H    QCI12  LIV_53     140000  12.304007  BL LASER -35163  -8699   \n",
      "12   146D0S    QCI12  LIV_53     150000  12.606833  BL LASER  40041  -5366   \n",
      "9    138C2G    QCI12  LIV_53     160000  12.497060  BL LASER -45429  -2366   \n",
      "13   147K45    QCI12  LIV_53     170000  12.520665  BL LASER  45687    601   \n",
      "10   138K5U    QCI12  LIV_53     180000  12.477738  BL LASER -51076   3568   \n",
      "14   148L7I    QCI12  LIV_53     190000  12.795605  BL LASER  55954   6568   \n",
      "15   156J1T    QCI12  LIV_53     200000  13.078524  BL LASER -61343   9901   \n",
      "17   167C36    QCI12  LIV_53     210000  13.424617  BL LASER  62884  12268   \n",
      "16   156L4U    QCI12  LIV_53     220000  13.119354  BL LASER -62626  15268   \n",
      "18   167J6H    QCI12  LIV_53     230000  13.101508  BL LASER  57237  18301   \n",
      "20   176L0S    QCI12  LIV_53     240000  12.613684  BL LASER -52359  21634   \n",
      "23   185I2G    QCI12  LIV_53     250000  12.706607  BL LASER  46971  24634   \n",
      "19   176E45    QCI12  LIV_53     260000  12.515595  BL LASER -46713  27601   \n",
      "22   184C5U    QCI12  LIV_53     270000  12.739930  BL LASER  41324  30568   \n",
      "21   177F7I    QCI12  LIV_53     280000  12.470641  BL LASER -36446  33568   \n",
      "25   203M2T    QCI12  LIV_53     290000  12.506005  BL LASER  33367  38701   \n",
      "24   200E56    QCI12  LIV_53     300000  12.258280  BL LASER   7444  42868   \n",
      "26   215K0G    QCI12  LIV_53     310000  12.943946  BL LASER -40809  48034   \n",
      "27   219K2T    QCI12  LIV_53     320000  12.333542  BL LASER   2567  52201   \n",
      "28   222G56    QCI12  LIV_53     330000  13.466911  BL LASER  38244  56368   \n",
      "11   140J05    QCI12  LIV_53     340000  12.268603      SILC -29003  -6599   \n",
      "\n",
      "    FLAG  \n",
      "29  None  \n",
      "30  None  \n",
      "32  None  \n",
      "31  None  \n",
      "33  None  \n",
      "2   None  \n",
      "3   None  \n",
      "0   None  \n",
      "4   None  \n",
      "1   None  \n",
      "8   None  \n",
      "6   None  \n",
      "7   None  \n",
      "5   None  \n",
      "12  None  \n",
      "9   None  \n",
      "13  None  \n",
      "10  None  \n",
      "14  None  \n",
      "15  None  \n",
      "17  None  \n",
      "16  None  \n",
      "18  None  \n",
      "20  None  \n",
      "23  None  \n",
      "19  None  \n",
      "22  None  \n",
      "21  None  \n",
      "25  None  \n",
      "24  None  \n",
      "26  None  \n",
      "27  None  \n",
      "28  None  \n",
      "11  None  \n"
     ]
    }
   ],
   "source": [
    "def find_ith_value(intensity, current, min_slope_fitpnt=1, max_slope_fitpnt=10, window_length=5, polyorder=2):\n",
    "    try:\n",
    "        # Sort data by current to ensure proper processing\n",
    "        sorted_indices = np.argsort(current)\n",
    "        current, intensity = current[sorted_indices], intensity[sorted_indices]\n",
    "\n",
    "        # Normalize intensity using provided max and min PD values\n",
    "        min_intensity, max_intensity = np.min(intensity), np.max(intensity)\n",
    "        intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        # Normalize intensity using min-max scaling\n",
    "        min_intensity = np.min(intensity)\n",
    "        max_intensity = np.max(intensity)\n",
    "        intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        # Apply Savitzky-Golay smoothing to normalized intensity\n",
    "        smoothed_intensity_norm = savgol_filter(intensity_norm, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "        # Compute differentials on normalized & smoothed data\n",
    "        smoothed_dI_dC_norm = np.gradient(smoothed_intensity_norm, current)\n",
    "        smoothed_d2I_dC2_norm = np.gradient(smoothed_dI_dC_norm, current)\n",
    "\n",
    "        # Filter the data to only consider LDI between 2 and 30 mA for Gaussian fitting\n",
    "        mask = (current >= 2) & (current <= 30)\n",
    "        if not np.any(mask):\n",
    "            print(\"Warning: No data points in the 2-30 mA range.\")\n",
    "            return None, None\n",
    "\n",
    "        current_masked = current[mask]\n",
    "        smoothed_d2I_dC2_norm_masked = smoothed_d2I_dC2_norm[mask]\n",
    "\n",
    "        # Fit Gaussian to the smoothed second differential\n",
    "        p0 = [np.max(smoothed_d2I_dC2_norm_masked), np.median(current_masked), np.std(current_masked)]\n",
    "        popt, pcov = curve_fit(gaussian, current_masked, smoothed_d2I_dC2_norm_masked, p0=p0)\n",
    "        median_x = popt[1]  # Extract median x from Gaussian fit\n",
    "\n",
    "        # Handle fitting errors\n",
    "        if not (2 <= median_x <= 30):\n",
    "            print(\"Warning: Gaussian fit unable to find reasonable split point. \")\n",
    "            median_x = 12.5\n",
    "        elif np.any(np.diag(pcov) > 0.1):  # Adjust threshold as needed\n",
    "            print(\"Warning: Abnormal LI curve detected due to high error in Gaussian fit.\")\n",
    "            return None, None\n",
    "\n",
    "        # Split data at median_x\n",
    "        left_side = current[current <= median_x]\n",
    "        right_side_mask = (current > median_x + min_slope_fitpnt) & (\n",
    "            current < median_x + max_slope_fitpnt\n",
    "        )  # Only Fit Right Slope after a few current pnts after ITH, and up to a certain current maximum above ITH.\n",
    "        right_side = current[right_side_mask]\n",
    "        intensity_norm_left = intensity_norm[current <= median_x]\n",
    "        intensity_norm_right = intensity_norm[right_side_mask]\n",
    "\n",
    "        # Check if either side is empty\n",
    "        if len(left_side) == 0 or len(right_side) == 0:\n",
    "            print(\"Warning: No reasonable I_th detected within bounds.\")\n",
    "            return None, None\n",
    "\n",
    "        # Fit linear regression to both segments\n",
    "        slope_left, intercept_left, _, _, _ = linregress(left_side, intensity_norm_left)\n",
    "        slope_right, intercept_right, _, _, _ = linregress(right_side, intensity_norm_right)\n",
    "\n",
    "        # Compute intersection point\n",
    "        intersection_x = (intercept_right - intercept_left) / (slope_left - slope_right)\n",
    "        ith_value = intersection_x  # No rounding\n",
    "\n",
    "        # Final evaluation check for ITH value\n",
    "        if not (2 <= ith_value <= 30):\n",
    "            print(\"Warning: Computed ITH value outside valid bounds (2-30 mA). Returning None.\")\n",
    "            return None, slope_right\n",
    "\n",
    "        return ith_value, slope_right\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Gaussian model for fitting (including offset)\n",
    "def gaussian(x, a, x0, sigma):\n",
    "    return a * np.exp(-((x - x0) ** 2) / (2 * sigma**2))\n",
    "\n",
    "\n",
    "# Linear model for line fitting\n",
    "def linear_model(x, slope, intercept):\n",
    "    return slope * x + intercept\n",
    "\n",
    "\n",
    "# Least Absolute Residuals fitting function using L1 norm\n",
    "def least_absolute_residuals_fit(x, y, model, initial_guess, bounds):\n",
    "    def objective(params):\n",
    "        return np.sum(np.abs(model(x, *params) - y))\n",
    "\n",
    "    result = minimize(objective, initial_guess, bounds=bounds, method=\"L-BFGS-B\", options={\"maxiter\": 1000})\n",
    "\n",
    "    residuals = np.abs(model(x, *result.x) - y)\n",
    "    mean_abs_error = np.mean(residuals)\n",
    "\n",
    "    return result.x, mean_abs_error\n",
    "\n",
    "\n",
    "def find_ith_value_labview(intensity, current):\n",
    "    # try:\n",
    "    # 1) Trim data to only include values >2 and <=35 mA\n",
    "    mask_trimmed = (current > 2) & (current <= 35)\n",
    "    if not np.any(mask_trimmed):\n",
    "        print(\"Warning: No data points between 2 and 35 mA.\")\n",
    "        return 0, 0\n",
    "    current = current[mask_trimmed]\n",
    "    intensity = intensity[mask_trimmed]\n",
    "\n",
    "    # 2) Interpolate to double resolution (spacing of 0.5 mA)\n",
    "    current_interp = np.arange(np.min(current), np.max(current) + 0.1, 0.5)\n",
    "    intensity_interp = np.interp(current_interp, current, intensity)\n",
    "\n",
    "    current = current_interp\n",
    "    intensity = intensity_interp\n",
    "\n",
    "    # 3) Normalize intensity using min-max scaling (first operation)\n",
    "    min_intensity = np.min(intensity)\n",
    "    max_intensity = np.max(intensity)\n",
    "    intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "    # 4) Sort data by current to ensure proper processing\n",
    "    sorted_indices = np.argsort(current)\n",
    "    current = current[sorted_indices]\n",
    "    intensity_norm = intensity_norm[sorted_indices]\n",
    "\n",
    "    # 5) Apply Chebyshev high-pass filter (order 2, ripple 0.1 dB, bandpass 0.15–0.45)\n",
    "    b, a = cheby1(N=2, rp=0.1, Wn=[0.15, 0.45], btype=\"bandpass\", fs=1)\n",
    "    filtered_intensity = filtfilt(b, a, intensity_norm)\n",
    "\n",
    "    # 5a) Initial linear fit via least absolute residuals on filtered data using QuantReg\n",
    "    X = sm.add_constant(current)  # Adds intercept term\n",
    "    model = sm.QuantReg(filtered_intensity, X)\n",
    "    res = model.fit(q=0.5)\n",
    "    slope_left = res.params[1]\n",
    "    intercept_left = res.params[0]\n",
    "    initial_abs_residual_total = np.mean(np.abs(filtered_intensity - res.predict(X)))\n",
    "\n",
    "    if initial_abs_residual_total > 0.1:\n",
    "        print(\"Warning: Initial L1 fit residual too high.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # 6) First Savitzky-Golay smoothing (5,1)\n",
    "    smoothed_intensity = savgol_filter(filtered_intensity, window_length=5, polyorder=1)\n",
    "\n",
    "    # 7) Second Savitzky-Golay smoothing before differential (3,2)\n",
    "    smoothed_intensity = savgol_filter(smoothed_intensity, window_length=3, polyorder=2)\n",
    "\n",
    "    # 8) Compute first derivative (renamed to dL_dI)\n",
    "    dL_dI = np.gradient(smoothed_intensity, current)\n",
    "\n",
    "    # 9) Smooth first derivative (6,2)\n",
    "    smoothed_dL_dI = savgol_filter(dL_dI, window_length=6, polyorder=2)\n",
    "\n",
    "    # 10) Compute second derivative (renamed to d2L_dI2)\n",
    "    d2L_dI2 = np.gradient(smoothed_dL_dI, current)\n",
    "\n",
    "    # 11) Smooth second derivative (6,2)\n",
    "    smoothed_d2L_dI2 = savgol_filter(d2L_dI2, window_length=6, polyorder=2)\n",
    "\n",
    "    # 11a) Set negative second derivative values to zero (LabVIEW-like behavior)\n",
    "    smoothed_d2L_dI2[smoothed_d2L_dI2 < 0] = 0\n",
    "\n",
    "    # 12) Normalize second derivative and add 0.01\n",
    "    max_d2L_dI2 = np.max(smoothed_d2L_dI2)\n",
    "    if max_d2L_dI2 == 0:\n",
    "        print(\"Warning: Second derivative all zero after zeroing negatives.\")\n",
    "        return 0, 0\n",
    "    d2L_dI2_ready = (smoothed_d2L_dI2 / max_d2L_dI2) + 0.01\n",
    "\n",
    "    # 13) Least Absolute Residuals fitting with initial conditions and bounds\n",
    "    initial_guess = [1, 11, np.std(current)]\n",
    "    bounds = [\n",
    "        (0, np.inf),  # Bounds for parameter a\n",
    "        (7, np.max(current) - 3),  # Bounds for parameter x0\n",
    "        (0, 3),  # Bounds for parameter sigma\n",
    "    ]\n",
    "    popt, _ = least_absolute_residuals_fit(current, d2L_dI2_ready, gaussian, initial_guess, bounds)\n",
    "    median_x = popt[1]\n",
    "\n",
    "    # 14) Validate split point\n",
    "    if not (2 <= median_x <= 35):\n",
    "        print(\"Warning: Gaussian fit split point out of bounds. Using default 12.5 mA.\")\n",
    "        median_x = 12.5\n",
    "\n",
    "    # 15) Linear fitting on left segment (with gradient bound)\n",
    "    left_mask = current <= median_x\n",
    "    if not np.any(left_mask):\n",
    "        print(\"Warning: No data points on the left segment for fitting.\")\n",
    "        return 0, 0\n",
    "\n",
    "    current_left = current[left_mask]\n",
    "    intensity_left = intensity_norm[left_mask]\n",
    "\n",
    "    popt_left, _ = curve_fit(linear_model, current_left, intensity_left, bounds=([0.001, -np.inf], [np.inf, np.inf]))\n",
    "    slope_left, intercept_left = popt_left\n",
    "\n",
    "    # 16) Linear fitting on right segment (no bounds)\n",
    "    right_mask = current > median_x\n",
    "    if not np.any(right_mask):\n",
    "        print(\"Warning: No data points on the right segment for fitting.\")\n",
    "        return 0, 0\n",
    "\n",
    "    current_right = current[right_mask]\n",
    "    intensity_right = intensity_norm[right_mask]\n",
    "\n",
    "    if len(current_right) < 10:\n",
    "        print(\"Warning: Fewer than 10 data points for stimulated emission fit.\")\n",
    "        return 0, 0\n",
    "\n",
    "    popt_right, _ = curve_fit(linear_model, current_right, intensity_right)\n",
    "    slope_efficiency, intercept_right = popt_right\n",
    "\n",
    "    fitted_right = linear_model(current_right, *popt_right)\n",
    "    mse_right = np.mean((intensity_right - fitted_right) ** 2)\n",
    "\n",
    "    if mse_right > 0.1:\n",
    "        print(\"Warning: High MSE in stimulated emission fit.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # 17) Compute intersection (I_th)\n",
    "    ith_value = (intercept_right - intercept_left) / (slope_left - slope_efficiency)\n",
    "    if not (2 <= ith_value <= 35):\n",
    "        print(\"Warning: Computed I_th outside bounds. Returning None.\")\n",
    "        return 0, 0\n",
    "\n",
    "    return ith_value, slope_efficiency\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "#     return 0, 0\n",
    "\n",
    "\n",
    "def evaluate_ITH_rawsweep(df_raw_sweeps, touchdown_number, sampling_rate=1):\n",
    "    df_raw_sweeps[\"ITH\"] = np.nan\n",
    "    for touchdown in range(\n",
    "        sampling_rate, touchdown_number * sampling_rate + 1, sampling_rate\n",
    "    ):  # accounts for selecting every nth touchdown if sampling.\n",
    "        specific_data = df_raw_sweeps[df_raw_sweeps[\"TOUCHDOWN\"] == touchdown]\n",
    "        if not specific_data.empty and \"NO LASER\" not in specific_data[\"FLAG\"].values:\n",
    "            ith_value, slope_efficiency = find_ith_value_labview(\n",
    "                specific_data[\"PD\"].values,\n",
    "                specific_data[\"LDI_mA\"].values,\n",
    "            )\n",
    "            if ith_value != 0:\n",
    "                df_raw_sweeps.loc[df_raw_sweeps[\"TOUCHDOWN\"] == touchdown, \"ITH\"] = ith_value\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "def generate_ITH_device_summary_table(raw_sweeps):\n",
    "\n",
    "    # Create COD Summary table with POT_FAILMODE\n",
    "    device_summary = (\n",
    "        (\n",
    "            raw_sweeps.groupby(\"TE_LABEL\").agg(\n",
    "                {\n",
    "                    \"WAFER_ID\": \"first\",\n",
    "                    \"MACH\": \"first\",\n",
    "                    \"TOUCHDOWN\": \"first\",\n",
    "                    \"ITH\": \"first\",\n",
    "                    \"TYPE\": \"first\",\n",
    "                    \"X_UM\": \"first\",\n",
    "                    \"Y_UM\": \"first\",\n",
    "                    \"FLAG\": \"first\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"TOUCHDOWN\")\n",
    "    )\n",
    "\n",
    "    return device_summary\n",
    "\n",
    "\n",
    "annotated_sweeps_tables = []\n",
    "\n",
    "\n",
    "device_summary_tables = []\n",
    "\n",
    "\n",
    "wafer_summary_tables = []\n",
    "\n",
    "\n",
    "for df_raw_sweeps, num_devices, sampling_rate, wafer_code in zip(\n",
    "    raw_sweeps_tables, device_numbers, sampling_rates, wafer_codes\n",
    "):\n",
    "    # Apply the NO LASER flag function\n",
    "    df_raw_sweeps = flag_no_laser_touchdowns(df_raw_sweeps)\n",
    "    # print(f\"\\nflagged:\\n {df_raw_sweeps}\")\n",
    "\n",
    "    # Run ITH evaluations\n",
    "    df_raw_sweeps = evaluate_ITH_rawsweep(df_raw_sweeps, num_devices, sampling_rate)\n",
    "    # print(f\"\\nannotated:\\n {df_raw_sweeps}\")\n",
    "    annotated_sweeps_tables.append(df_raw_sweeps)\n",
    "    # df_raw_sweeps.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_raw_sweeps.csv\", index=False)\n",
    "\n",
    "    # Device Summary\n",
    "    device_summary = generate_ITH_device_summary_table(df_raw_sweeps)\n",
    "    print(f\"\\ndevice summary:\\n {device_summary}\")\n",
    "    device_summary_tables.append(device_summary)\n",
    "    device_summary.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_device_summary.csv\", index=False)\n",
    "\n",
    "# print(annotated_sweeps_tables[0].head(1000))\n",
    "\n",
    "# print(device_summary_tables[0].head(1000))\n",
    "# print(len(device_summary_tables))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "df_combined = pd.concat(device_summary_tables, ignore_index=True)\n",
    "df_combined.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_device_combined_summary.csv\", index=False)\n",
    "\n",
    "# Display the first 10 rows of the combined dataframe\n",
    "# print(df_combined.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Sweep Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annotated_sweeps_tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Find the correct dataframe where the wafer code matches the input\u001b[39;00m\n\u001b[32m    202\u001b[39m df_raw_sweeps = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mannotated_sweeps_tables\u001b[49m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mWAFER_ID\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m] == WAFER_CODE:\n\u001b[32m    205\u001b[39m         df_raw_sweeps = df\n",
      "\u001b[31mNameError\u001b[39m: name 'annotated_sweeps_tables' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to plot PD/LDI and Vf/LDI for a specific laser and wafer\n",
    "def plot_specific_touchdown(df_raw_sweeps, wafer_code, touchdown, pnt_size):\n",
    "    specific_data = df_raw_sweeps[(df_raw_sweeps[\"WAFER_ID\"] == wafer_code) & (df_raw_sweeps[\"TOUCHDOWN\"] == touchdown)]\n",
    "\n",
    "    if specific_data.empty:\n",
    "        print(f\"No data found for Wafer Code: {wafer_code} and TOUCHDOWN: {touchdown}\")\n",
    "        return\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "    # Plot PD/LDI\n",
    "    ax1.scatter(specific_data[\"LDI_mA\"], specific_data[\"PD\"], s=pnt_size, color=\"blue\")\n",
    "    ax1.set_title(f\"{wafer_code}: Scatter Plot of PD vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax1.set_xlabel(\"LDI_mA\")\n",
    "    ax1.set_ylabel(\"PD\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot dP/dI\n",
    "    ax3.scatter(specific_data[\"LDI_mA\"], specific_data[\"dP/dI\"], s=pnt_size, color=\"blue\")\n",
    "    ax3.set_title(f\"{wafer_code}: Scatter Plot of dP/dI for TOUCHDOWN {touchdown}\")\n",
    "    ax3.set_xlabel(\"LDI_mA\")\n",
    "    ax3.set_ylabel(\"dP/dI\")\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plot d2P/dI2\n",
    "    ax5.scatter(specific_data[\"LDI_mA\"], specific_data[\"d2P/dI2\"], s=pnt_size, color=\"blue\")\n",
    "    ax5.set_title(f\"{wafer_code}: Scatter Plot of d2P/dI2 for TOUCHDOWN {touchdown}\")\n",
    "    ax5.set_xlabel(\"LDI_mA\")\n",
    "    ax5.set_ylabel(\"d2P/dI2\")\n",
    "    ax5.grid(True)\n",
    "\n",
    "    # Plot Vf/LDI\n",
    "    ax2.scatter(specific_data[\"LDI_mA\"], specific_data[\"Vf\"], s=pnt_size, color=\"green\")\n",
    "    ax2.set_title(f\"{wafer_code}: Scatter Plot of Vf vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax2.set_xlabel(\"LDI_mA\")\n",
    "    ax2.set_ylabel(\"Vf\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot dV/dI\n",
    "    ax4.scatter(specific_data[\"LDI_mA\"], specific_data[\"dV/dI\"], s=pnt_size, color=\"green\")\n",
    "    ax4.set_title(f\"{wafer_code}: Scatter Plot of dV/dI vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax4.set_xlabel(\"LDI_mA\")\n",
    "    ax4.set_ylabel(\"dV/dI\")\n",
    "    ax4.grid(True)\n",
    "\n",
    "    # Plot d2V/dI2\n",
    "    ax6.scatter(specific_data[\"LDI_mA\"], specific_data[\"d2V/dI2\"], s=pnt_size, color=\"green\")\n",
    "    ax6.set_title(f\"{wafer_code}: Scatter Plot of d2V/dI2 vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax6.set_xlabel(\"LDI_mA\")\n",
    "    ax6.set_ylabel(\"d2V/dI2\")\n",
    "    ax6.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gaussian(x, a, mu, sigma):\n",
    "    \"\"\"Gaussian function for curve fitting.\"\"\"\n",
    "    return a * np.exp(-((x - mu) ** 2) / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def plot_with_smoothing_and_normalization(\n",
    "    df_raw_sweeps, wafer_code, touchdown, window_length=5, polyorder=2, max_slope_fitpnt=20\n",
    "):\n",
    "    specific_data = df_raw_sweeps[(df_raw_sweeps[\"WAFER_ID\"] == wafer_code) & (df_raw_sweeps[\"TOUCHDOWN\"] == touchdown)]\n",
    "\n",
    "    if specific_data.empty:\n",
    "        print(f\"No data found for Wafer Code: {wafer_code} and TOUCHDOWN: {touchdown}\")\n",
    "        return\n",
    "\n",
    "    # Sort data by LDI to ensure proper plotting\n",
    "    specific_data = specific_data.sort_values(by=\"LDI_mA\")\n",
    "\n",
    "    # Normalize PD using min-max scaling\n",
    "    min_PD = specific_data[\"PD\"].min()\n",
    "    max_PD = specific_data[\"PD\"].max()\n",
    "    specific_data[\"PD_norm\"] = (specific_data[\"PD\"] - min_PD) / (max_PD - min_PD)\n",
    "\n",
    "    # Apply Savitzky-Golay smoothing to normalized PD\n",
    "    smoothed_PD_norm = savgol_filter(specific_data[\"PD_norm\"], window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "    # Compute differentials on normalized & smoothed data\n",
    "    dP_dI_norm = np.gradient(specific_data[\"PD_norm\"], specific_data[\"LDI_mA\"])\n",
    "    smoothed_dP_dI_norm = np.gradient(smoothed_PD_norm, specific_data[\"LDI_mA\"])\n",
    "\n",
    "    d2P_dI2_norm = np.gradient(dP_dI_norm, specific_data[\"LDI_mA\"])\n",
    "    smoothed_d2P_dI2_norm = np.gradient(smoothed_dP_dI_norm, specific_data[\"LDI_mA\"])\n",
    "\n",
    "    # Fit Gaussian to the smoothed second differential\n",
    "    try:\n",
    "        mask = (specific_data[\"LDI_mA\"] >= 2) & (specific_data[\"LDI_mA\"] <= 30)\n",
    "        current_masked = specific_data[\"LDI_mA\"][mask]\n",
    "        smoothed_d2P_dI2_norm_masked = smoothed_d2P_dI2_norm[mask]\n",
    "\n",
    "        p0 = [np.max(smoothed_d2P_dI2_norm_masked), np.median(current_masked), np.std(current_masked)]\n",
    "        popt, _ = curve_fit(gaussian, current_masked, smoothed_d2P_dI2_norm_masked, p0=p0)\n",
    "        gaussian_fit = gaussian(specific_data[\"LDI_mA\"], *popt)\n",
    "        median_x = popt[1]\n",
    "    except:\n",
    "        median_x = np.nan\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 20))  # Extra row for line fitting plot\n",
    "\n",
    "    # Normalized PD vs LDI (Unsmoothed)\n",
    "    axes[0, 0].plot(specific_data[\"LDI_mA\"], specific_data[\"PD_norm\"], color=\"blue\", label=\"Normalized PD (Raw)\")\n",
    "    axes[0, 0].set_title(f\"{wafer_code}: PD vs LDI (Normalized, Unsmoothed)\")\n",
    "    axes[0, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[0, 0].set_ylabel(\"Normalized PD\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Normalized PD vs LDI (Smoothed)\n",
    "    axes[0, 1].plot(specific_data[\"LDI_mA\"], smoothed_PD_norm, color=\"red\", label=\"Normalized PD (Smoothed)\")\n",
    "    axes[0, 1].set_title(f\"{wafer_code}: PD vs LDI (Normalized & Smoothed)\")\n",
    "    axes[0, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[0, 1].set_ylabel(\"Normalized PD\")\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Normalized dP/dI (Unsmoothed)\n",
    "    axes[1, 0].plot(specific_data[\"LDI_mA\"], dP_dI_norm, color=\"blue\", label=\"dP/dI (Raw)\")\n",
    "    axes[1, 0].set_title(f\"{wafer_code}: dP/dI (Normalized, Unsmoothed)\")\n",
    "    axes[1, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[1, 0].set_ylabel(\"dP/dI\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Normalized dP/dI (Smoothed)\n",
    "    axes[1, 1].plot(specific_data[\"LDI_mA\"], smoothed_dP_dI_norm, color=\"red\", label=\"dP/dI (Smoothed)\")\n",
    "    axes[1, 1].set_title(f\"{wafer_code}: dP/dI (Normalized & Smoothed)\")\n",
    "    axes[1, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[1, 1].set_ylabel(\"dP/dI\")\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # Normalized d2P/dI2 (Unsmoothed)\n",
    "    axes[2, 0].plot(specific_data[\"LDI_mA\"], d2P_dI2_norm, color=\"blue\", label=\"d2P/dI2 (Raw)\")\n",
    "    axes[2, 0].set_title(f\"{wafer_code}: d2P/dI2 (Normalized, Unsmoothed)\")\n",
    "    axes[2, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[2, 0].set_ylabel(\"d2P/dI2\")\n",
    "    axes[2, 0].grid(True)\n",
    "\n",
    "    # Normalized d2P/dI2 (Smoothed) + Gaussian Fit\n",
    "    axes[2, 1].plot(specific_data[\"LDI_mA\"], smoothed_d2P_dI2_norm, color=\"red\", label=\"d2P/dI2 (Smoothed)\")\n",
    "    axes[2, 1].plot(specific_data[\"LDI_mA\"], gaussian_fit, color=\"purple\", linestyle=\"dashed\", label=\"Gaussian Fit\")\n",
    "\n",
    "    # Mark median_x with a vertical line\n",
    "    if not np.isnan(median_x):\n",
    "        axes[2, 1].axvline(median_x, color=\"black\", linestyle=\"dotted\", label=f\"Median: {median_x:.3f}\")\n",
    "\n",
    "    axes[2, 1].set_title(f\"{wafer_code}: d2P/dI2 (Normalized & Smoothed) + Gaussian Fit\")\n",
    "    axes[2, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[2, 1].set_ylabel(\"d2P/dI2\")\n",
    "    axes[2, 1].grid(True)\n",
    "    axes[2, 1].legend()\n",
    "\n",
    "    # ------------------- LINEAR FITTING SECTION (Separate Figure) -------------------\n",
    "    if not np.isnan(median_x):\n",
    "        # Split data at median_x\n",
    "        left_side = specific_data[specific_data[\"LDI_mA\"] <= median_x]\n",
    "        right_side_mask = (specific_data[\"LDI_mA\"] > median_x) & (specific_data[\"LDI_mA\"] < max_slope_fitpnt)\n",
    "        right_side = specific_data[right_side_mask]\n",
    "\n",
    "        if not left_side.empty and not right_side.empty:\n",
    "            # Fit linear regression to both segments\n",
    "            slope_left, intercept_left, _, _, _ = linregress(left_side[\"LDI_mA\"], left_side[\"PD_norm\"])\n",
    "            slope_right, intercept_right, _, _, _ = linregress(right_side[\"LDI_mA\"], right_side[\"PD_norm\"])\n",
    "\n",
    "            # Generate fitted lines\n",
    "            fit_left = slope_left * left_side[\"LDI_mA\"] + intercept_left\n",
    "            fit_right = slope_right * right_side[\"LDI_mA\"] + intercept_right\n",
    "\n",
    "            # Compute intersection point\n",
    "            intersection_x = (intercept_right - intercept_left) / (slope_left - slope_right)\n",
    "            intersection_y = slope_left * intersection_x + intercept_left\n",
    "\n",
    "        # Create a separate figure for linear fits\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot original normalized PD data as scatter points\n",
    "        plt.scatter(\n",
    "            specific_data[\"LDI_mA\"], specific_data[\"PD_norm\"], color=\"green\", marker=\"+\", alpha=1, label=\"Original Data\"\n",
    "        )\n",
    "\n",
    "        # Plot linear fits\n",
    "        plt.plot(left_side[\"LDI_mA\"], fit_left, color=\"blue\", label=f\"Left Fit (Slope={slope_left:.3f})\")\n",
    "        plt.plot(right_side[\"LDI_mA\"], fit_right, color=\"red\", label=f\"Right Fit (Slope={slope_right:.3f})\")\n",
    "\n",
    "        # Mark the intersection point\n",
    "        plt.scatter(\n",
    "            intersection_x,\n",
    "            intersection_y,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            s=200,\n",
    "            label=f\"Intersection at LDI={intersection_x:.3f}\",\n",
    "        )\n",
    "\n",
    "        # Labels and title\n",
    "        plt.title(f\"{wafer_code}: Linear Fits Split at Median\")\n",
    "        plt.xlabel(\"LDI_mA\")\n",
    "        plt.ylabel(\"Normalized PD\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# INPUT THE DESIRED PROFILE TO EXAMINE HERE\n",
    "# Define the specific wafer code and TOUCHDOWN number\n",
    "WAFER_CODE = \"QCHZZ\"\n",
    "TOUCHDOWN = 20\n",
    "\n",
    "# Find the correct dataframe where the wafer code matches the input\n",
    "df_raw_sweeps = None\n",
    "for df in annotated_sweeps_tables:\n",
    "    if df[\"WAFER_ID\"].iloc[0] == WAFER_CODE:\n",
    "        df_raw_sweeps = df\n",
    "        break\n",
    "\n",
    "if df_raw_sweeps is not None:\n",
    "    # Plot for the specified touchdown number\n",
    "    # plot_specific_touchdown(df_raw_sweeps, WAFER_CODE, TOUCHDOWN, pnt_size=5)\n",
    "    plot_with_smoothing_and_normalization(df_raw_sweeps, WAFER_CODE, TOUCHDOWN, window_length=5, polyorder=2)\n",
    "    ITH_value = find_ith_value(df_raw_sweeps, \"QCHZZ\", TOUCHDOWN)\n",
    "    print(f\"ITH value: {ITH_value}\")\n",
    "else:\n",
    "    print(f\"No data found for Wafer Code: {WAFER_CODE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
