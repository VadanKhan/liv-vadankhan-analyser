{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy.stats as st\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "import sys\n",
    "import time\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "# Move to the root directory\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "# Add the root directory to the system path\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Import the importlib module\n",
    "import importlib\n",
    "\n",
    "# import function implementations\n",
    "import stst_urls\n",
    "\n",
    "# Reload the modules\n",
    "importlib.reload(stst_urls)\n",
    "\n",
    "# Re-import the functions\n",
    "from stst_urls import GTX_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileserver link: https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/\n",
      "['https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCI12/LIV_53_QCI12_DNS-LIVTKCOD_LIVTK-DNS_RAW20250319065143.CSV']\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\n",
    "    # \"QCHZZ\",\n",
    "    # \"QCHZ2\",\n",
    "    # \"QCHYZ\",\n",
    "    # \"QCI04\",\n",
    "    # \"QCHYL\",\n",
    "    # \"QCI06\",\n",
    "    \"QCI12\",\n",
    "    # \"QCI09\",\n",
    "    # \"QCHZB\",\n",
    "    # \"QCHZR\",\n",
    "    # \"QCI0O\",\n",
    "]  # List of wafer codes\n",
    "\n",
    "# Recent Good Wafers\n",
    "# QCHZ2\n",
    "# QCHYZ\n",
    "# QCI04\n",
    "# QCHYL\n",
    "# QCHWP\n",
    "# QCI06\n",
    "# QCI12\n",
    "# QCI09\n",
    "# QCHZB\n",
    "# QCHZR\n",
    "# QCI0O\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"QCI12_newITH_study\"\n",
    "\n",
    "DECODER_FILE = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "DECODER_FILE_PATH = ROOT_DIR / \"decoders\" / DECODER_FILE\n",
    "RESULTS_FILE_PATH = ROOT_DIR / \"results\"\n",
    "\n",
    "EXPORTS_FILEPATH = ROOT_DIR / \"exports\"\n",
    "# Create the exports folder if it doesn't exist\n",
    "if not os.path.exists(EXPORTS_FILEPATH):\n",
    "    os.makedirs(EXPORTS_FILEPATH)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def liv_raw_filelink_finder(wafer_codes, fileserver_link: str, product_code=\"QC\"):\n",
    "    fileserver_link = f\"{fileserver_link}{product_code}/\"\n",
    "    print(f\"fileserver link: {fileserver_link}\")\n",
    "\n",
    "    response = requests.get(fileserver_link, verify=False)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    subdirectory_urls = []\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        if href and any(wafer_code in href for wafer_code in wafer_codes):\n",
    "            subdirectory_urls.append(fileserver_link + href)\n",
    "\n",
    "    file_urls = []\n",
    "    file_cod_urls = []\n",
    "    file_degradation_urls = []\n",
    "    machine_list = []\n",
    "    machine_dict = {}\n",
    "\n",
    "    file_times = []\n",
    "    file_cod_times = []\n",
    "    file_degradation_times = []\n",
    "\n",
    "    for wafer_code, subdirectory_url in zip(wafer_codes, subdirectory_urls):\n",
    "        response = requests.get(subdirectory_url, verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "\n",
    "        latest_file = None\n",
    "        latest_cod_file = None\n",
    "        latest_degradation_file = None\n",
    "\n",
    "        latest_time = \"\"\n",
    "        latest_cod_time = \"\"\n",
    "        latest_degradation_time = \"\"\n",
    "        machine_name = None\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href and \"RAW\" in href:\n",
    "                time_str = href[-18:-4]  # Extract the time string\n",
    "\n",
    "                if not machine_name:\n",
    "                    machine_name = href[:6]  # Extract the machine name (first 6 characters)\n",
    "\n",
    "                if \"COD250\" in href:\n",
    "                    if time_str > latest_cod_time:\n",
    "                        latest_cod_time = time_str\n",
    "                        latest_cod_file = subdirectory_url + href\n",
    "                elif \"COD70\" in href:\n",
    "                    if time_str > latest_degradation_time:\n",
    "                        latest_degradation_time = time_str\n",
    "                        latest_degradation_file = subdirectory_url + href\n",
    "                else:\n",
    "                    if time_str > latest_time:\n",
    "                        latest_time = time_str\n",
    "                        latest_file = subdirectory_url + href\n",
    "\n",
    "        if latest_file:\n",
    "            file_urls.append(latest_file)\n",
    "            file_times.append(latest_time)\n",
    "        if latest_cod_file:\n",
    "            file_cod_urls.append(latest_cod_file)\n",
    "            file_cod_times.append(latest_cod_time)\n",
    "        if latest_degradation_file:\n",
    "            file_degradation_urls.append(latest_degradation_file)\n",
    "            file_degradation_times.append(latest_degradation_time)\n",
    "        if machine_name:\n",
    "            machine_list.append(machine_name)\n",
    "            machine_dict[wafer_code] = machine_name\n",
    "\n",
    "    return (file_urls, file_cod_urls, file_degradation_urls, file_times, file_cod_times, file_degradation_times, machine_list, machine_dict)\n",
    "\n",
    "\n",
    "# Calling code\n",
    "file_urls, file_cod_urls, file_degradation_urls, file_times, file_cod_times, file_degradation_times, machine_list, machine_dict = liv_raw_filelink_finder(wafer_codes, GTX_URL, \"QC\")\n",
    "print(file_urls)\n",
    "\n",
    "# DEBUG: INPUT LINKS TO OTHER GTX FILES HERE\n",
    "# file_urls = [\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_DNS-LIVTKCOD_LCRVCOD250-DNS_RAW20250227044906.CSV\",\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_LIVBLTKCOD_COD250-DNS_RAW20250228082707.CSV\",\n",
    "#     \"https://sprgtxprod02.stni.seagate.com/~gtx/wafer/proc_LIV/data/byProdLot/QC/QCHWQ/LIV_53_QCHWQ_LIVBLTKCOD_COD250-DNS_RAW20250311164324.CSV\",\n",
    "# ]\n",
    "# print(file_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data to Desired Raw Sweep Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selects required columns\n",
    "- transposes\n",
    "- stacks data in tall format\n",
    "- adds in device coords from decoder file\n",
    "- loops for every csv file chosen, and stores raw_sweep dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Reading the data rows, skipping the header rows...\n",
      "Step 2 completed in 9.64 seconds\n",
      "Step 3: Filtering every 10000th laser...\n",
      "Step 3 completed in 0.01 seconds\n",
      "Step 4: Subsetting the data frame...\n",
      "Step 4 completed in 0.00 seconds\n",
      "Step 5: Transposing the data frame...\n",
      "Step 5 completed in 0.01 seconds\n",
      "Step 6: Splitting the transposed table...\n",
      "Step 6 completed in 0.01 seconds\n",
      "Step 7: Learning data dimensions...\n",
      "Number of Current Measurements per Device: 63\n",
      "Number of Devices: 34\n",
      "Step 7 completed in 0.00 seconds\n",
      "Step 8: Concatenating Voltage columns...\n",
      "Step 8 completed in 0.00 seconds\n",
      "Step 9: Concatenating PD columns...\n",
      "Step 9 completed in 0.00 seconds\n",
      "Step 10: Performing Cartesian join...\n",
      "Step 10 completed in 0.00 seconds\n",
      "Step 11: Adding device coordinates...\n",
      "Step 11 completed in 0.02 seconds\n",
      "Step 12: Merging with decoder file...\n",
      "Step 12 completed in 1.42 seconds\n",
      "Step 13: Renaming columns...\n",
      "Step 13 completed in 0.00 seconds\n",
      "Step 14: Adding current column...\n",
      "Step 14 completed in 0.00 seconds\n",
      "Step 15: Adding WAFER_ID column...\n",
      "Step 15 completed in 0.00 seconds\n",
      "Step 16: Adding MACHINE_CODE column...\n",
      "Step 16 completed in 0.00 seconds\n",
      "Total time taken: 11.12 seconds\n"
     ]
    }
   ],
   "source": [
    "def transform_raw_liv_file(file_url, decoder_file_path, machine_code, wafer_id):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the CSV file from the URL, skipping the first 19 rows\n",
    "    print(\"Step 1: Reading the CSV file...\")\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        skiprows=19,\n",
    "    )\n",
    "    print(f\"Step 1 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 3: Subsetting the data frame...\")\n",
    "    col_names = df.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Transpose the data frame and reset index\n",
    "    print(\"Step 4: Transposing the data frame...\")\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 5: Splitting the transposed table...\")\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Learn data dimensions\n",
    "    print(\"Step 6: Learning data dimensions...\")\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Concatenate all Voltage columns into one\n",
    "    print(\"Step 7: Concatenating Voltage columns...\")\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = [i // n_meas + 1 for i in range(n_meas * n_devices)]\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all PD columns into one\n",
    "    print(\"Step 8: Concatenating PD columns...\")\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 9: Performing Cartesian join...\")\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Add device coordinates from original RAW file\n",
    "    print(\"Step 10: Adding device coordinates...\")\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\")\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 11: Merging with decoder file...\")\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Rename the columns\n",
    "    print(\"Step 12: Renaming columns...\")\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 13: Adding current column...\")\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 14: Adding WAFER_ID column...\")\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 15: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "def transform_raw_liv_file_first_n_rows(file_url, decoder_file_path, machine_code, wafer_id, n):\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    # Step 2: Read the data rows, skipping the header rows\n",
    "    print(\"Step 2: Reading the data rows, skipping the header rows...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_url, skiprows=19, nrows=n)\n",
    "    print(f\"Step 2 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 3: Subsetting the data frame...\")\n",
    "    start_time = time.time()\n",
    "    col_names = df.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Transpose the data frame and reset index\n",
    "    print(\"Step 4: Transposing the data frame...\")\n",
    "    start_time = time.time()\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 5: Splitting the transposed table...\")\n",
    "    start_time = time.time()\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Learn data dimensions\n",
    "    print(\"Step 6: Learning data dimensions...\")\n",
    "    start_time = time.time()\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Concatenate all Voltage columns into one\n",
    "    print(\"Step 7: Concatenating Voltage columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = [i // n_meas + 1 for i in range(n_meas * n_devices)]\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all PD columns into one\n",
    "    print(\"Step 8: Concatenating PD columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 9: Performing Cartesian join...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Add device coordinates from original RAW file\n",
    "    print(\"Step 10: Adding device coordinates...\")\n",
    "    start_time = time.time()\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\")\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 11: Merging with decoder file...\")\n",
    "    start_time = time.time()\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Rename the columns\n",
    "    print(\"Step 12: Renaming columns...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 13: Adding current column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 14: Adding WAFER_ID column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 15: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time_overall\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    sampling_rate = 1\n",
    "\n",
    "    return (df_raw_sweeps, n_meas, n_devices, sampling_rate)\n",
    "\n",
    "\n",
    "def transform_raw_liv_file_every_nth_laser(file_url, decoder_file_path, machine_code, wafer_id, n=10000):\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    # Step 2: Read the data rows, skipping the header rows\n",
    "    print(\"Step 2: Reading the data rows, skipping the header rows...\")\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_url, skiprows=19)\n",
    "    print(f\"Step 2 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Filter every nth laser based on TOUCHDOWN\n",
    "    print(f\"Step 3: Filtering every {n}th laser...\")\n",
    "    start_time = time.time()\n",
    "    df_filtered = df[df[\"TOUCHDOWN\"] % n == 0]\n",
    "    print(f\"Step 3 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 4: Get column names and subset the data frame with selected columns\n",
    "    print(\"Step 4: Subsetting the data frame...\")\n",
    "    start_time = time.time()\n",
    "    col_names = df_filtered.columns\n",
    "    selected_cols = [col for col in col_names if \"Vf\" in col or \"PD\" in col]\n",
    "    df_subset = df_filtered[selected_cols]\n",
    "    cols_to_delete = [col for col in df_subset.columns if \"Vf@\" in col or \"PD@\" in col]\n",
    "    df_subset.drop(columns=cols_to_delete, inplace=True)\n",
    "    print(f\"Step 4 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 5: Transpose the data frame and reset index\n",
    "    print(\"Step 5: Transposing the data frame...\")\n",
    "    start_time = time.time()\n",
    "    df_transposed = df_subset.transpose()\n",
    "    df_transposed.reset_index(inplace=True)\n",
    "    new_columns = [\"Label\"] + list(range(1, len(df_transposed.columns)))\n",
    "    df_transposed.columns = new_columns\n",
    "    df_transposed.loc[-1] = new_columns  # Add the new row at the top\n",
    "    df_transposed.index = df_transposed.index + 1  # Shift the index\n",
    "    df_transposed = df_transposed.sort_index()  # Sort by index to place the new row at the top\n",
    "    print(f\"Step 5 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 6: Split transposed table into Vf and PD data tables\n",
    "    print(\"Step 6: Splitting the transposed table...\")\n",
    "    start_time = time.time()\n",
    "    df_vf = df_transposed[df_transposed[\"Label\"].str.contains(\"Vf\")]\n",
    "    df_pd = df_transposed[df_transposed[\"Label\"].str.contains(\"PD\")]\n",
    "    df_vf.drop(columns=[\"Label\"], inplace=True)\n",
    "    df_pd.drop(columns=[\"Label\"], inplace=True)\n",
    "    print(f\"Step 6 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 7: Learn data dimensions\n",
    "    print(\"Step 7: Learning data dimensions...\")\n",
    "    start_time = time.time()\n",
    "    n_meas = df_vf.shape[0]\n",
    "    print(f\"Number of Current Measurements per Device: {n_meas}\")\n",
    "    n_devices = df_vf.shape[1]\n",
    "    print(f\"Number of Devices: {n_devices}\")\n",
    "    print(f\"Step 7 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Concatenate all Voltage columns into one\n",
    "    print(\"Step 8: Concatenating Voltage columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_vf = pd.concat([df_vf[col] for col in df_vf.columns], ignore_index=True).to_frame(name=\"Vf\")\n",
    "    # Instead of generating a new TOUCHDOWN, reuse the one from df_filtered\n",
    "    df_concat_vf[\"TOUCHDOWN\"] = df_filtered[\"TOUCHDOWN\"].repeat(n_meas).values\n",
    "    print(f\"Step 8 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 9: Concatenate all PD columns into one\n",
    "    print(\"Step 9: Concatenating PD columns...\")\n",
    "    start_time = time.time()\n",
    "    df_concat_pd = pd.concat([df_pd[col] for col in df_pd.columns], ignore_index=True).to_frame(name=\"PD\")\n",
    "    print(f\"Step 9 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 10: Cartesian join of Vf and PD data tables\n",
    "    print(\"Step 10: Performing Cartesian join...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps = pd.concat([df_concat_vf, df_concat_pd], axis=1)\n",
    "    print(f\"Step 10 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 11: Add device coordinates from original RAW file\n",
    "    print(\"Step 11: Adding device coordinates...\")\n",
    "    start_time = time.time()\n",
    "    if \"TOUCHDOWN\" in df.columns and \"STX_WAFER_X_UM\" in df.columns and \"STX_WAFER_Y_UM\" in df.columns:\n",
    "        df_raw_sweeps = df_raw_sweeps.merge(df[[\"TOUCHDOWN\", \"STX_WAFER_X_UM\", \"STX_WAFER_Y_UM\"]], on=\"TOUCHDOWN\", how=\"left\")\n",
    "    else:\n",
    "        print(\"Required columns for merging device coordinates are missing in the original RAW file.\")\n",
    "    print(f\"Step 11 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 12: Merge with decoder file to get TE_LABEL etc.\n",
    "    print(\"Step 12: Merging with decoder file...\")\n",
    "    start_time = time.time()\n",
    "    if decoder_file_path.exists():\n",
    "        df_decoder = pd.read_csv(decoder_file_path)\n",
    "        if \"YMIN\" in df_decoder.columns and \"XMIN\" in df_decoder.columns:\n",
    "            df_raw_sweeps = df_raw_sweeps.merge(\n",
    "                df_decoder[[\"YMIN\", \"XMIN\", \"TE_LABEL\", \"TYPE\"]],\n",
    "                left_on=[\"STX_WAFER_Y_UM\", \"STX_WAFER_X_UM\"],\n",
    "                right_on=[\"YMIN\", \"XMIN\"],\n",
    "                how=\"left\",\n",
    "            ).drop(columns=[\"YMIN\", \"XMIN\"])\n",
    "        else:\n",
    "            print(\"Required columns for merging decoder data are missing in the decoder file.\")\n",
    "    else:\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "    print(f\"Step 12 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 13: Rename the columns\n",
    "    print(\"Step 13: Renaming columns...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.rename(columns={\"STX_WAFER_X_UM\": \"X_UM\", \"STX_WAFER_Y_UM\": \"Y_UM\"}, inplace=True)\n",
    "    print(f\"Step 13 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 14: Add current column as a repeating sequence of length n_meas\n",
    "    print(\"Step 14: Adding current column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps[\"LDI_mA\"] = [i % n_meas + 1 for i in range(len(df_raw_sweeps))]\n",
    "    print(f\"Step 14 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 15: Add a column for WAFER_ID with the wafer_id value repeated for every row\n",
    "    print(\"Step 15: Adding WAFER_ID column...\")\n",
    "    start_time = time.time()\n",
    "    df_raw_sweeps.insert(0, \"WAFER_ID\", wafer_id)\n",
    "    print(f\"Step 15 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Step 16: Add a column for machine_code with the machine_code value repeated for every row\n",
    "    print(\"Step 16: Adding MACHINE_CODE column...\")\n",
    "    df_raw_sweeps.insert(0, \"MACH\", machine_code)\n",
    "    print(f\"Step 16 completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    total_time = time.time() - start_time_overall\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    sampling_rate = n\n",
    "\n",
    "    return (df_raw_sweeps, n_meas, n_devices, sampling_rate)\n",
    "\n",
    "\n",
    "raw_sweeps_tables = []\n",
    "device_numbers = []\n",
    "sampling_rates = []\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROW_NUMBER = 1000\n",
    "\n",
    "# # CALLING THE CODE\n",
    "# for file_url, machine_code in zip(file_urls, machine_list):\n",
    "#     df_raw_sweeps, n_meas, n_devices = transform_raw_liv_file_first_n_rows(file_url, DECODER_FILE_PATH, machine_code, ROW_NUMBER)\n",
    "#     if df_raw_sweeps[\"TE_LABEL\"].isna().any():\n",
    "#         raise ValueError(\"ERROR: Decoder Matching Failed! Perhaps the wrong decoder file was used, no matching X or Y coords found\")\n",
    "#     raw_sweeps_tables.append(df_raw_sweeps)\n",
    "#     device_numbers.append(n_devices)\n",
    "\n",
    "# # Display the first 10 rows of the raw_sweeps table\n",
    "# print(raw_sweeps_tables[0].head(10))\n",
    "# print(device_numbers[0])\n",
    "\n",
    "# DEBUG: CALLING SAMPLED DATA ACROSS MULTIPLE WAFERS\n",
    "SAMPLE_ROWS = 10000  # You can change this value to any other number as needed\n",
    "for file_url, machine_code, wafer_code in zip(file_urls, machine_list, wafer_codes):\n",
    "    df_raw_sweeps, n_meas, n_devices, sampling_rate = transform_raw_liv_file_every_nth_laser(file_url, DECODER_FILE_PATH, machine_code, wafer_code, n=SAMPLE_ROWS)\n",
    "    if df_raw_sweeps[\"TE_LABEL\"].isna().any():\n",
    "        raise ValueError(\"ERROR: Decoder Matching Failed! Perhaps the wrong decoder file was used, no matching X or Y coords found\")\n",
    "    raw_sweeps_tables.append(df_raw_sweeps)\n",
    "    device_numbers.append(n_devices)\n",
    "    sampling_rates.append(sampling_rate)\n",
    "\n",
    "# # Concatenate all dataframes together\n",
    "# df_combined = pd.concat(raw_sweeps_tables, ignore_index=True)\n",
    "# # easy measure to make rest of code call:\n",
    "# raw_sweeps_tables = [df_combined]\n",
    "\n",
    "# # Display the first 10 rows of the combined dataframe\n",
    "# print(df_combined.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, a, x0, sigma):\n",
    "    return a * np.exp(-((x - x0) ** 2) / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def basic_sweep_analysis(df):\n",
    "    \"\"\"\n",
    "    Compute first and second order differentials for voltage (Vf) and photodiode signal (PD)\n",
    "    while ensuring calculations remain per device.\n",
    "    Additionally, compute min and max PD per touchdown and clone max PD across the sweep.\n",
    "    \"\"\"\n",
    "    df[\"dV/dI\"] = df.groupby(\"TOUCHDOWN\")[\"Vf\"].diff()\n",
    "    df[\"dP/dI\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].diff()\n",
    "    df[\"d2V/dI2\"] = df.groupby(\"TOUCHDOWN\")[\"dV/dI\"].diff()\n",
    "    df[\"d2P/dI2\"] = df.groupby(\"TOUCHDOWN\")[\"dP/dI\"].diff()\n",
    "\n",
    "    df[\"MAX_PD\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].transform(\"max\")\n",
    "    df[\"MIN_PD\"] = df.groupby(\"TOUCHDOWN\")[\"PD\"].transform(\"min\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def flag_no_laser_touchdowns(df_raw_sweeps):\n",
    "    \"\"\"\n",
    "    Adds a \"FLAG\" column to df_raw_sweeps, labeling touchdowns as \"NO LASER\"\n",
    "    if the max PD value for that touchdown is below 1.\n",
    "    \"\"\"\n",
    "    df_raw_sweeps[\"FLAG\"] = np.nan\n",
    "    no_laser_touchdowns = df_raw_sweeps.groupby(\"TOUCHDOWN\")[\"PD\"].max()\n",
    "    no_laser_touchdowns = no_laser_touchdowns[no_laser_touchdowns < 1].index\n",
    "    df_raw_sweeps.loc[df_raw_sweeps[\"TOUCHDOWN\"].isin(no_laser_touchdowns), \"FLAG\"] = \"NO LASER\"\n",
    "    return df_raw_sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I_th Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def find_ith_value(intensity, current, min_slope_fitpnt=1, max_slope_fitpnt=10, window_length=5, polyorder=2):\n",
    "    try:\n",
    "        # Sort data by current to ensure proper processing\n",
    "        sorted_indices = np.argsort(current)\n",
    "        current, intensity = current[sorted_indices], intensity[sorted_indices]\n",
    "\n",
    "        # Normalize intensity using provided max and min PD values\n",
    "        min_intensity, max_intensity = np.min(intensity), np.max(intensity)\n",
    "        intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        # Normalize intensity using min-max scaling\n",
    "        min_intensity = np.min(intensity)\n",
    "        max_intensity = np.max(intensity)\n",
    "        intensity_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        # Apply Savitzky-Golay smoothing to normalized intensity\n",
    "        smoothed_intensity_norm = savgol_filter(intensity_norm, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "        # Compute differentials on normalized & smoothed data\n",
    "        smoothed_dI_dC_norm = np.gradient(smoothed_intensity_norm, current)\n",
    "        smoothed_d2I_dC2_norm = np.gradient(smoothed_dI_dC_norm, current)\n",
    "\n",
    "        # Filter the data to only consider LDI between 2 and 30 mA for Gaussian fitting\n",
    "        mask = (current >= 2) & (current <= 30)\n",
    "        if not np.any(mask):\n",
    "            print(\"Warning: No data points in the 2-30 mA range.\")\n",
    "            return None, None\n",
    "\n",
    "        current_masked = current[mask]\n",
    "        smoothed_d2I_dC2_norm_masked = smoothed_d2I_dC2_norm[mask]\n",
    "\n",
    "        # Fit Gaussian to the smoothed second differential\n",
    "        p0 = [np.max(smoothed_d2I_dC2_norm_masked), np.median(current_masked), np.std(current_masked)]\n",
    "        popt, pcov = curve_fit(gaussian, current_masked, smoothed_d2I_dC2_norm_masked, p0=p0)\n",
    "        median_x = popt[1]  # Extract median x from Gaussian fit\n",
    "\n",
    "        # Handle fitting errors\n",
    "        if not (2 <= median_x <= 30):\n",
    "            print(\"Warning: Gaussian fit unable to find reasonable split point. \")\n",
    "            median_x = 12.5\n",
    "        elif np.any(np.diag(pcov) > 0.1):  # Adjust threshold as needed\n",
    "            print(\"Warning: Abnormal LI curve detected due to high error in Gaussian fit.\")\n",
    "            return None, None\n",
    "\n",
    "        # Split data at median_x\n",
    "        left_side = current[current <= median_x]\n",
    "        right_side_mask = (current > median_x + min_slope_fitpnt) & (\n",
    "            current < median_x + max_slope_fitpnt\n",
    "        )  # Only Fit Right Slope after a few current pnts after ITH, and up to a certain current maximum above ITH.\n",
    "        right_side = current[right_side_mask]\n",
    "        intensity_norm_left = intensity_norm[current <= median_x]\n",
    "        intensity_norm_right = intensity_norm[right_side_mask]\n",
    "\n",
    "        # Check if either side is empty\n",
    "        if len(left_side) == 0 or len(right_side) == 0:\n",
    "            print(\"Warning: No reasonable I_th detected within bounds.\")\n",
    "            return None, None\n",
    "\n",
    "        # Fit linear regression to both segments\n",
    "        slope_left, intercept_left, _, _, _ = linregress(left_side, intensity_norm_left)\n",
    "        slope_right, intercept_right, _, _, _ = linregress(right_side, intensity_norm_right)\n",
    "\n",
    "        # Compute intersection point\n",
    "        intersection_x = (intercept_right - intercept_left) / (slope_left - slope_right)\n",
    "        ith_value = intersection_x  # No rounding\n",
    "\n",
    "        # Final evaluation check for ITH value\n",
    "        if not (2 <= ith_value <= 30):\n",
    "            print(\"Warning: Computed ITH value outside valid bounds (2-30 mA). Returning None.\")\n",
    "            return None, slope_right\n",
    "\n",
    "        return ith_value, slope_right\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def raw_sweep_ITH_evaluations(df_raw_sweeps, touchdown_number, sampling_rate=1):\n",
    "    df_raw_sweeps[\"ITH\"] = np.nan\n",
    "    for touchdown in range(sampling_rate, touchdown_number * sampling_rate + 1, sampling_rate):  # accounts for selecting every nth touchdown if sampling.\n",
    "        specific_data = df_raw_sweeps[df_raw_sweeps[\"TOUCHDOWN\"] == touchdown]\n",
    "        if not specific_data.empty and \"NO LASER\" not in specific_data[\"FLAG\"].values:\n",
    "            ith_value, _ = find_ith_value(\n",
    "                specific_data[\"PD\"].values,\n",
    "                specific_data[\"LDI_mA\"].values,\n",
    "            )\n",
    "            if ith_value is not None:\n",
    "                df_raw_sweeps.loc[df_raw_sweeps[\"TOUCHDOWN\"] == touchdown, \"ITH\"] = ith_value\n",
    "    return df_raw_sweeps\n",
    "\n",
    "\n",
    "def generate_ITH_device_summary_table(raw_sweeps):\n",
    "\n",
    "    # Create COD Summary table with POT_FAILMODE\n",
    "    device_summary = (\n",
    "        (\n",
    "            raw_sweeps.groupby(\"TE_LABEL\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"WAFER_ID\": \"first\",\n",
    "                    \"MACH\": \"first\",\n",
    "                    \"TOUCHDOWN\": \"first\",\n",
    "                    \"ITH\": \"first\",\n",
    "                    \"TYPE\": \"first\",\n",
    "                    \"X_UM\": \"first\",\n",
    "                    \"Y_UM\": \"first\",\n",
    "                    \"FLAG\": \"first\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        .sort_values(\"TOUCHDOWN\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return device_summary\n",
    "\n",
    "\n",
    "annotated_sweeps_tables = []\n",
    "\n",
    "\n",
    "device_summary_tables = []\n",
    "\n",
    "\n",
    "wafer_summary_tables = []\n",
    "\n",
    "\n",
    "for df_raw_sweeps, num_devices, sampling_rate, wafer_code in zip(raw_sweeps_tables, device_numbers, sampling_rates, wafer_codes):\n",
    "    # Apply the NO LASER flag function\n",
    "    df_raw_sweeps = flag_no_laser_touchdowns(df_raw_sweeps)\n",
    "    # print(f\"\\nflagged:\\n {df_raw_sweeps}\")\n",
    "\n",
    "    # Run ITH evaluations\n",
    "    df_raw_sweeps = raw_sweep_ITH_evaluations(df_raw_sweeps, num_devices, sampling_rate)\n",
    "    # print(f\"\\nannotated:\\n {df_raw_sweeps}\")\n",
    "    annotated_sweeps_tables.append(df_raw_sweeps)\n",
    "    df_raw_sweeps.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_raw_sweeps.csv\", index=False)\n",
    "\n",
    "    # Device Summary\n",
    "    device_summary = generate_ITH_device_summary_table(df_raw_sweeps)\n",
    "    device_summary_tables.append(device_summary)\n",
    "    device_summary.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_device_summary.csv\", index=False)\n",
    "\n",
    "    # # Adding the differentials and max and mins AT THE END.\n",
    "    # df_raw_sweeps = basic_sweep_analysis(df_raw_sweeps)\n",
    "\n",
    "# print(annotated_sweeps_tables[0].head(1000))\n",
    "\n",
    "# print(device_summary_tables[0].head(1000))\n",
    "\n",
    "# Concatenate all dataframes together\n",
    "print(len(device_summary_tables))\n",
    "df_combined = pd.concat(device_summary_tables, ignore_index=True)\n",
    "df_combined.to_csv(EXPORTS_FILEPATH / f\"{ANALYSIS_RUN_NAME}_device_combined_summary.csv\", index=False)\n",
    "\n",
    "# Display the first 10 rows of the combined dataframe\n",
    "# print(df_combined.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Sweep Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annotated_sweeps_tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Find the correct dataframe where the wafer code matches the input\u001b[39;00m\n\u001b[32m    202\u001b[39m df_raw_sweeps = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mannotated_sweeps_tables\u001b[49m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mWAFER_ID\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m] == WAFER_CODE:\n\u001b[32m    205\u001b[39m         df_raw_sweeps = df\n",
      "\u001b[31mNameError\u001b[39m: name 'annotated_sweeps_tables' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to plot PD/LDI and Vf/LDI for a specific laser and wafer\n",
    "def plot_specific_touchdown(df_raw_sweeps, wafer_code, touchdown, pnt_size):\n",
    "    specific_data = df_raw_sweeps[(df_raw_sweeps[\"WAFER_ID\"] == wafer_code) & (df_raw_sweeps[\"TOUCHDOWN\"] == touchdown)]\n",
    "\n",
    "    if specific_data.empty:\n",
    "        print(f\"No data found for Wafer Code: {wafer_code} and TOUCHDOWN: {touchdown}\")\n",
    "        return\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "    # Plot PD/LDI\n",
    "    ax1.scatter(specific_data[\"LDI_mA\"], specific_data[\"PD\"], s=pnt_size, color=\"blue\")\n",
    "    ax1.set_title(f\"{wafer_code}: Scatter Plot of PD vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax1.set_xlabel(\"LDI_mA\")\n",
    "    ax1.set_ylabel(\"PD\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot dP/dI\n",
    "    ax3.scatter(specific_data[\"LDI_mA\"], specific_data[\"dP/dI\"], s=pnt_size, color=\"blue\")\n",
    "    ax3.set_title(f\"{wafer_code}: Scatter Plot of dP/dI for TOUCHDOWN {touchdown}\")\n",
    "    ax3.set_xlabel(\"LDI_mA\")\n",
    "    ax3.set_ylabel(\"dP/dI\")\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plot d2P/dI2\n",
    "    ax5.scatter(specific_data[\"LDI_mA\"], specific_data[\"d2P/dI2\"], s=pnt_size, color=\"blue\")\n",
    "    ax5.set_title(f\"{wafer_code}: Scatter Plot of d2P/dI2 for TOUCHDOWN {touchdown}\")\n",
    "    ax5.set_xlabel(\"LDI_mA\")\n",
    "    ax5.set_ylabel(\"d2P/dI2\")\n",
    "    ax5.grid(True)\n",
    "\n",
    "    # Plot Vf/LDI\n",
    "    ax2.scatter(specific_data[\"LDI_mA\"], specific_data[\"Vf\"], s=pnt_size, color=\"green\")\n",
    "    ax2.set_title(f\"{wafer_code}: Scatter Plot of Vf vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax2.set_xlabel(\"LDI_mA\")\n",
    "    ax2.set_ylabel(\"Vf\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot dV/dI\n",
    "    ax4.scatter(specific_data[\"LDI_mA\"], specific_data[\"dV/dI\"], s=pnt_size, color=\"green\")\n",
    "    ax4.set_title(f\"{wafer_code}: Scatter Plot of dV/dI vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax4.set_xlabel(\"LDI_mA\")\n",
    "    ax4.set_ylabel(\"dV/dI\")\n",
    "    ax4.grid(True)\n",
    "\n",
    "    # Plot d2V/dI2\n",
    "    ax6.scatter(specific_data[\"LDI_mA\"], specific_data[\"d2V/dI2\"], s=pnt_size, color=\"green\")\n",
    "    ax6.set_title(f\"{wafer_code}: Scatter Plot of d2V/dI2 vs LDI_mA for TOUCHDOWN {touchdown}\")\n",
    "    ax6.set_xlabel(\"LDI_mA\")\n",
    "    ax6.set_ylabel(\"d2V/dI2\")\n",
    "    ax6.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gaussian(x, a, mu, sigma):\n",
    "    \"\"\"Gaussian function for curve fitting.\"\"\"\n",
    "    return a * np.exp(-((x - mu) ** 2) / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def plot_with_smoothing_and_normalization(df_raw_sweeps, wafer_code, touchdown, window_length=5, polyorder=2, max_slope_fitpnt=20):\n",
    "    specific_data = df_raw_sweeps[(df_raw_sweeps[\"WAFER_ID\"] == wafer_code) & (df_raw_sweeps[\"TOUCHDOWN\"] == touchdown)]\n",
    "\n",
    "    if specific_data.empty:\n",
    "        print(f\"No data found for Wafer Code: {wafer_code} and TOUCHDOWN: {touchdown}\")\n",
    "        return\n",
    "\n",
    "    # Sort data by LDI to ensure proper plotting\n",
    "    specific_data = specific_data.sort_values(by=\"LDI_mA\")\n",
    "\n",
    "    # Normalize PD using min-max scaling\n",
    "    min_PD = specific_data[\"PD\"].min()\n",
    "    max_PD = specific_data[\"PD\"].max()\n",
    "    specific_data[\"PD_norm\"] = (specific_data[\"PD\"] - min_PD) / (max_PD - min_PD)\n",
    "\n",
    "    # Apply Savitzky-Golay smoothing to normalized PD\n",
    "    smoothed_PD_norm = savgol_filter(specific_data[\"PD_norm\"], window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "    # Compute differentials on normalized & smoothed data\n",
    "    dP_dI_norm = np.gradient(specific_data[\"PD_norm\"], specific_data[\"LDI_mA\"])\n",
    "    smoothed_dP_dI_norm = np.gradient(smoothed_PD_norm, specific_data[\"LDI_mA\"])\n",
    "\n",
    "    d2P_dI2_norm = np.gradient(dP_dI_norm, specific_data[\"LDI_mA\"])\n",
    "    smoothed_d2P_dI2_norm = np.gradient(smoothed_dP_dI_norm, specific_data[\"LDI_mA\"])\n",
    "\n",
    "    # Fit Gaussian to the smoothed second differential\n",
    "    try:\n",
    "        mask = (specific_data[\"LDI_mA\"] >= 2) & (specific_data[\"LDI_mA\"] <= 30)\n",
    "        current_masked = specific_data[\"LDI_mA\"][mask]\n",
    "        smoothed_d2P_dI2_norm_masked = smoothed_d2P_dI2_norm[mask]\n",
    "\n",
    "        p0 = [np.max(smoothed_d2P_dI2_norm_masked), np.median(current_masked), np.std(current_masked)]\n",
    "        popt, _ = curve_fit(gaussian, current_masked, smoothed_d2P_dI2_norm_masked, p0=p0)\n",
    "        gaussian_fit = gaussian(specific_data[\"LDI_mA\"], *popt)\n",
    "        median_x = popt[1]\n",
    "    except:\n",
    "        median_x = np.nan\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 20))  # Extra row for line fitting plot\n",
    "\n",
    "    # Normalized PD vs LDI (Unsmoothed)\n",
    "    axes[0, 0].plot(specific_data[\"LDI_mA\"], specific_data[\"PD_norm\"], color=\"blue\", label=\"Normalized PD (Raw)\")\n",
    "    axes[0, 0].set_title(f\"{wafer_code}: PD vs LDI (Normalized, Unsmoothed)\")\n",
    "    axes[0, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[0, 0].set_ylabel(\"Normalized PD\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Normalized PD vs LDI (Smoothed)\n",
    "    axes[0, 1].plot(specific_data[\"LDI_mA\"], smoothed_PD_norm, color=\"red\", label=\"Normalized PD (Smoothed)\")\n",
    "    axes[0, 1].set_title(f\"{wafer_code}: PD vs LDI (Normalized & Smoothed)\")\n",
    "    axes[0, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[0, 1].set_ylabel(\"Normalized PD\")\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Normalized dP/dI (Unsmoothed)\n",
    "    axes[1, 0].plot(specific_data[\"LDI_mA\"], dP_dI_norm, color=\"blue\", label=\"dP/dI (Raw)\")\n",
    "    axes[1, 0].set_title(f\"{wafer_code}: dP/dI (Normalized, Unsmoothed)\")\n",
    "    axes[1, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[1, 0].set_ylabel(\"dP/dI\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Normalized dP/dI (Smoothed)\n",
    "    axes[1, 1].plot(specific_data[\"LDI_mA\"], smoothed_dP_dI_norm, color=\"red\", label=\"dP/dI (Smoothed)\")\n",
    "    axes[1, 1].set_title(f\"{wafer_code}: dP/dI (Normalized & Smoothed)\")\n",
    "    axes[1, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[1, 1].set_ylabel(\"dP/dI\")\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # Normalized d2P/dI2 (Unsmoothed)\n",
    "    axes[2, 0].plot(specific_data[\"LDI_mA\"], d2P_dI2_norm, color=\"blue\", label=\"d2P/dI2 (Raw)\")\n",
    "    axes[2, 0].set_title(f\"{wafer_code}: d2P/dI2 (Normalized, Unsmoothed)\")\n",
    "    axes[2, 0].set_xlabel(\"LDI_mA\")\n",
    "    axes[2, 0].set_ylabel(\"d2P/dI2\")\n",
    "    axes[2, 0].grid(True)\n",
    "\n",
    "    # Normalized d2P/dI2 (Smoothed) + Gaussian Fit\n",
    "    axes[2, 1].plot(specific_data[\"LDI_mA\"], smoothed_d2P_dI2_norm, color=\"red\", label=\"d2P/dI2 (Smoothed)\")\n",
    "    axes[2, 1].plot(specific_data[\"LDI_mA\"], gaussian_fit, color=\"purple\", linestyle=\"dashed\", label=\"Gaussian Fit\")\n",
    "\n",
    "    # Mark median_x with a vertical line\n",
    "    if not np.isnan(median_x):\n",
    "        axes[2, 1].axvline(median_x, color=\"black\", linestyle=\"dotted\", label=f\"Median: {median_x:.3f}\")\n",
    "\n",
    "    axes[2, 1].set_title(f\"{wafer_code}: d2P/dI2 (Normalized & Smoothed) + Gaussian Fit\")\n",
    "    axes[2, 1].set_xlabel(\"LDI_mA\")\n",
    "    axes[2, 1].set_ylabel(\"d2P/dI2\")\n",
    "    axes[2, 1].grid(True)\n",
    "    axes[2, 1].legend()\n",
    "\n",
    "    # ------------------- LINEAR FITTING SECTION (Separate Figure) -------------------\n",
    "    if not np.isnan(median_x):\n",
    "        # Split data at median_x\n",
    "        left_side = specific_data[specific_data[\"LDI_mA\"] <= median_x]\n",
    "        right_side_mask = (specific_data[\"LDI_mA\"] > median_x) & (specific_data[\"LDI_mA\"] < max_slope_fitpnt)\n",
    "        right_side = specific_data[right_side_mask]\n",
    "\n",
    "        if not left_side.empty and not right_side.empty:\n",
    "            # Fit linear regression to both segments\n",
    "            slope_left, intercept_left, _, _, _ = linregress(left_side[\"LDI_mA\"], left_side[\"PD_norm\"])\n",
    "            slope_right, intercept_right, _, _, _ = linregress(right_side[\"LDI_mA\"], right_side[\"PD_norm\"])\n",
    "\n",
    "            # Generate fitted lines\n",
    "            fit_left = slope_left * left_side[\"LDI_mA\"] + intercept_left\n",
    "            fit_right = slope_right * right_side[\"LDI_mA\"] + intercept_right\n",
    "\n",
    "            # Compute intersection point\n",
    "            intersection_x = (intercept_right - intercept_left) / (slope_left - slope_right)\n",
    "            intersection_y = slope_left * intersection_x + intercept_left\n",
    "\n",
    "        # Create a separate figure for linear fits\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot original normalized PD data as scatter points\n",
    "        plt.scatter(specific_data[\"LDI_mA\"], specific_data[\"PD_norm\"], color=\"green\", marker=\"+\", alpha=1, label=\"Original Data\")\n",
    "\n",
    "        # Plot linear fits\n",
    "        plt.plot(left_side[\"LDI_mA\"], fit_left, color=\"blue\", label=f\"Left Fit (Slope={slope_left:.3f})\")\n",
    "        plt.plot(right_side[\"LDI_mA\"], fit_right, color=\"red\", label=f\"Right Fit (Slope={slope_right:.3f})\")\n",
    "\n",
    "        # Mark the intersection point\n",
    "        plt.scatter(intersection_x, intersection_y, color=\"black\", marker=\"x\", s=200, label=f\"Intersection at LDI={intersection_x:.3f}\")\n",
    "\n",
    "        # Labels and title\n",
    "        plt.title(f\"{wafer_code}: Linear Fits Split at Median\")\n",
    "        plt.xlabel(\"LDI_mA\")\n",
    "        plt.ylabel(\"Normalized PD\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# INPUT THE DESIRED PROFILE TO EXAMINE HERE\n",
    "# Define the specific wafer code and TOUCHDOWN number\n",
    "WAFER_CODE = \"QCHZZ\"\n",
    "TOUCHDOWN = 20\n",
    "\n",
    "# Find the correct dataframe where the wafer code matches the input\n",
    "df_raw_sweeps = None\n",
    "for df in annotated_sweeps_tables:\n",
    "    if df[\"WAFER_ID\"].iloc[0] == WAFER_CODE:\n",
    "        df_raw_sweeps = df\n",
    "        break\n",
    "\n",
    "if df_raw_sweeps is not None:\n",
    "    # Plot for the specified touchdown number\n",
    "    # plot_specific_touchdown(df_raw_sweeps, WAFER_CODE, TOUCHDOWN, pnt_size=5)\n",
    "    plot_with_smoothing_and_normalization(df_raw_sweeps, WAFER_CODE, TOUCHDOWN, window_length=5, polyorder=2)\n",
    "    ITH_value = find_ith_value(df_raw_sweeps, \"QCHZZ\", TOUCHDOWN)\n",
    "    print(f\"ITH value: {ITH_value}\")\n",
    "else:\n",
    "    print(f\"No data found for Wafer Code: {WAFER_CODE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
